{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here just for simplicity, I write all preprocess code together. If you are instested what happend in the preprocess step, please move to this [notebook](https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-cnn/notebooks/char-level-text-preprocess-with-keras-summary.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# write all code in one cell \n",
    "\n",
    "#========================Load data=========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data_source = '../data/ag_news_csv/train.csv'\n",
    "test_data_source = '../data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_source, header=None)\n",
    "test_df = pd.read_csv(test_data_source, header=None)\n",
    "\n",
    "# concatenate column 1 and column 2 as one text\n",
    "for df in [train_df, test_df]:\n",
    "    df[1] = df[1] + df[2]\n",
    "    df = df.drop([2], axis=1)\n",
    "    \n",
    "# convert string to lower case \n",
    "train_texts = train_df[1].values \n",
    "train_texts = [s.lower() for s in train_texts] \n",
    "\n",
    "test_texts = test_df[1].values \n",
    "test_texts = [s.lower() for s in test_texts] \n",
    "\n",
    "#=======================Convert string to index================\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "#-----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary \n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict \n",
    "# Add 'UNK' to the vocabulary \n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "#-----------------------Skip part end----------------------------\n",
    "\n",
    "# Convert string to index \n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#=======================Get classes================\n",
    "train_classes = train_df[0].values\n",
    "train_class_list = [x-1 for x in train_classes]\n",
    "\n",
    "test_classes = test_df[0].values\n",
    "test_class_list = [x-1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(train_class_list)\n",
    "test_classes = to_categorical(test_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model\n",
    "\n",
    "We implement the char_cnn_zhang model from this paper:\n",
    "\n",
    "- Xiang Zhang, Junbo Zhao, Yann LeCun. [Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626). NIPS 2015\n",
    "\n",
    "The model structure:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*fovAEUSdSkbsnJw5.png)\n",
    "\n",
    "This graph may look difficult to understand. Here is the model setup. \n",
    "\n",
    "![](https://img-blog.csdn.net/20170721104727009).\n",
    "\n",
    "\n",
    "If you want to see the detail for this model, please move to this [notebook](https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-cnn/notebooks/char_cnn_zhang.ipynb)\n",
    "\n",
    "We choose the small frame, 256 filters in convolutional layer and 1024 output units in dense layer. \n",
    "\n",
    "- Embedding Layer\n",
    "- Six convolutional layers, and 3 convolutional layers followed by a max pooling layer\n",
    "- Two fully connected layer(dense layer in keras), neuron units are 1024. \n",
    "- Output layer(dense layer), neuron units depends on classes. In this task, we set it 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layer import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk.word_index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "embedding_weights = []\n",
    "tk.word_index "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
