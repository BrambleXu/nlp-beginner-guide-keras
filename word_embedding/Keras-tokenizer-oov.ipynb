{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the documentation about Tokenizer in Keras:\n",
    "\n",
    "```\n",
    "keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ \n",
    "', lower=True, split=' ', char_level=False, oov_token=None)\n",
    "```\n",
    "\n",
    "- `num_words`: the maximum number of words to keep, based on word frequency. Only the most common num_words words will be kept.\n",
    "\n",
    "According to the description for `num_words`, the vocabulary size should be `num_words`. But in practice, the vocabulary contains all words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 1, 'is': 2, 'my': 3, 'far': 4, 'faraway': 5, 'asdasd': 6, 'your': 7}\n",
      "[[1, 2], [1, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "tk = Tokenizer(num_words=num_words)\n",
    "texts = [\"my name is far faraway asdasd\", \"my name is\",\"your name is\"]\n",
    "tk.fit_on_texts(texts)\n",
    "print(tk.word_index)\n",
    "print(tk.texts_to_sequences(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tk.word_index` is the vocabulary contain all words even if we set `num_word=3`, but the index is ordered as the word frequency. \n",
    "\n",
    "- `0` is a reserved index, never assigned to an existing word. So if we want `texts_to_sequences` to output text with 3 most common words, we should set `num_word=3+1`. \n",
    "\n",
    "We also can set `oov_token='UNK'` to add `UNK` to the vocabulary. Here the index of UNK is `word_cont+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 1, 'is': 2, 'my': 3, 'far': 4, 'faraway': 5, 'asdasd': 6, 'your': 7, 'UNK': 8}\n",
      "[[3, 1, 2], [3, 1, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "tk = Tokenizer(num_words=num_words+1, oov_token='UNK')\n",
    "texts = [\"my name is far faraway asdasd\", \"my name is\",\"your name is\"]\n",
    "tk.fit_on_texts(texts)\n",
    "print(tk.word_index)\n",
    "print(tk.texts_to_sequences(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `texts_to_sequences` process each sentence, it only consider the top `num_word` in the dictionary. So the dictionary used by the `texts_to_sequences` is `{'name': 1, 'is': 2, 'my': 3}`. We can see, for the sentence 1 and sentence 3, if there are **UNKNOWN** words in the `{'name': 1, 'is': 2, 'my': 3}`, `texts_to_sequences` will just skip it. \n",
    "\n",
    "This is not well when dealing with the OOV(out of vocabulary). We hope the output format should like:\n",
    "\n",
    "```[[3, 1, 2, UNK, UNK, UNK], [3, 1, 2], [UNK, 1, 2]]```\n",
    "\n",
    "Here we change the `tk.word_index`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.oov_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.word_index = {e:i for e,i in tk.word_index.items() if i <= num_words} # <= because tokenizer is 1 indexed\n",
    "tk.word_index[tk.oov_token] = num_words + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 1, 'is': 2, 'my': 3, 'UNK': 4}\n",
      "[[3, 1, 2, 4, 4, 4], [3, 1, 2], [4, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(tk.word_index)\n",
    "print(tk.texts_to_sequences(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4, 4, 4], [3, 1, 2], [4, 1, 2]]\n",
      "[[3 1 2 4 4 4 0 0 0 0]\n",
      " [3 1 2 0 0 0 0 0 0 0]\n",
      " [4 1 2 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tk.texts_to_sequences(texts)\n",
    "print(sequences)\n",
    "data = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, if we use padding, we should set `mask=True` in the Embedding layer. If we use LSTM, it will ignore the padding part. And for the vector of UNKNOWN, we can not just set 0 because UNKNOWN also contains some information. There are some methods to try, averaging vectors for many infrequent words, or you can use a random vector. You can find more info [here](https://groups.google.com/d/msg/word2vec-toolkit/TgMeiJJGDc0/d1vueZkqeHIJ)\n",
    "\n",
    "Another refrence: https://github.com/keras-team/keras/issues/8092\n",
    "\n",
    "If we use tensorflow, we should use this to compute the mask: https://www.tensorflow.org/api_docs/python/tf/sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
