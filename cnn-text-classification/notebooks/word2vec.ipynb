{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "After the preprocess, we get the `x_train, y_train, x_test, y_test, vocabulary_inv` for later use. First we use word2vec model to train `x_train` to get the **word distributed representation**. You can take the **word distributed representation** as more powerful features for sentence. After we get these features, we will feed them to model.\n",
    "\n",
    "In this notebook, we write the preprocess part in one cell. If you forget, please see the **preprocess notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/xu/anaconda3/envs/tf/lib/python36.zip', '/Users/xu/anaconda3/envs/tf/lib/python3.6', '/Users/xu/anaconda3/envs/tf/lib/python3.6/lib-dynload', '/Users/xu/anaconda3/envs/tf/lib/python3.6/site-packages', '/Users/xu/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/extensions', '/Users/xu/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the `data_helpers` module to the path. Here we use `os.pardir` to represent the parent directry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n",
      "x_train shape:  (9595, 56)\n",
      "x_test shape: (1067, 56)\n",
      "Vocabulary Size: 18765\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "y = y.argmax(axis=1) # y: [1, 1, 1, ...., 0, 0, 0]. 1 for positive, 0 for negative\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "\n",
    "# Output shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Vocabulary Size: {:d}'.format(len(vocabulary_inv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper of [Convolutional Neural Networks for Sentence Classification](http://www.aclweb.org/anthology/D14-1181), the author proposed several CNN variants.\n",
    "\n",
    "* CNN-rand: No word2vec. All words vector are randomly initialized and then modified during training.\n",
    "\n",
    "* CNN-static: Pre-train a word2vec, but do not learn it during training. If a word dose not show in the word2vec, the unknown word vector are randomly initialized. \n",
    "\n",
    "* CNN-non-static: Same as above but the pretrained vectors are fine-tuned for each task.\n",
    "\n",
    "* CNN-multichannel: A model with two sets of word vectors(CNN-static, CNN-non-static). Each set of vectors is treated as a ‘channel’ and each filter is applied\n",
    "\n",
    "Here we choose CNN-non-static to implement. We need to add a embedding layer. We use `word2vec.py` to pre-train the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameter for training the word2vec model\n",
    "\"\"\"\n",
    "inputs:\n",
    "sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "vocabulary_inv  # dict {int: str}\n",
    "num_features    # Word vector dimensionality                      \n",
    "min_word_count  # Minimum word count                        \n",
    "context         # Context window size \n",
    "\"\"\"\n",
    "sentence_matrix = x\n",
    "# vocabulary_inv = vocabulary_inv\n",
    "num_features=300\n",
    "min_word_count=1\n",
    "context=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2  # Number of threads to run in parallel\n",
    "downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "# sample(param in gensim): threshold for configuring which \n",
    "# higher-frequency words are randomly downsampled;\n",
    "# default is 1e-3, values of 1e-5 (or lower) may also be useful, \n",
    "# set to 0.0 to disable downsampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'conan',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'arnold',\n",
       " 'schwarzenegger',\n",
       " ',',\n",
       " 'jean',\n",
       " 'claud',\n",
       " 'van',\n",
       " 'damme',\n",
       " 'or',\n",
       " 'steven',\n",
       " 'segal',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show words\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                    size=num_features, min_count=min_word_count,\n",
    "                                    window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/300features_1minwords_10context'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "model_dir = 'models'\n",
    "model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "model_name = join(model_dir, model_name)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400features\n",
      "400features\n"
     ]
    }
   ],
   "source": [
    "print(\"{:d}features\".format(400))\n",
    "print(\"{0:d}features\".format(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models', '300features_1minwords_10context')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Word2Vec '300features_1minwords_10context'\n"
     ]
    }
   ],
   "source": [
    "if not exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "print('Saving Word2Vec \\'%s\\'' % split(model_name)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.18361306 -0.02460535  0.08828513 -0.07919128  0.11477375  0.14191546\n",
      "  0.04934429 -0.0182813  -0.02350191  0.0839237 ]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# get the vector of word 'rock' \n",
    "print(embedding_model.wv['rock'].shape)\n",
    "print(embedding_model.wv['rock'][:10])\n",
    "print(embedding_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97043616,  0.02826277, -0.02718172, -0.08975446,  0.32522753,\n",
       "        0.49387988,  0.3853714 , -0.19174875, -0.09329756,  0.3320315 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.wv['<PAD/>'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if a word not in the embedding_model, we randomly initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unknown word vector\n",
    "embedding_weights = {}\n",
    "for key, word in vocabulary_inv.items():\n",
    "    if word in embedding_model.wv:\n",
    "        embedding_weights[key] = embedding_model.wv[word]\n",
    "    else:\n",
    "        embedding_weights[key] = np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary['rock'])\n",
    "print(vocabulary['<PAD/>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18361306 -0.02460535  0.08828513 -0.07919128  0.11477375  0.14191546\n",
      "  0.04934429 -0.0182813  -0.02350191  0.0839237 ]\n",
      "[-0.97043616  0.02826277 -0.02718172 -0.08975446  0.32522753  0.49387988\n",
      "  0.3853714  -0.19174875 -0.09329756  0.3320315 ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights[565][:10]) # rock vector\n",
    "print(embedding_weights[0][:10]) # <PAD/> vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
