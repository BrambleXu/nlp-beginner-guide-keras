{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from word2vec import train_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n",
      "x_train shape:  (9595, 56)\n",
      "x_test shape: (1067, 56)\n",
      "Vocabulary Size: 18765\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "y = y.argmax(axis=1) # y: [1, 1, 1, ...., 0, 0, 0]. 1 for positive, 0 for negative\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "\n",
    "# Output shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Vocabulary Size: {:d}'.format(len(vocabulary_inv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Word2Vec model '300feature_1minwords_10context'\n",
      "[ 0.09885646 -0.09516437 -0.05571043 -0.14348853  0.00774352  0.06113464\n",
      " -0.07387891 -0.14808127 -0.10660333  0.08960743  0.09765676  0.10535081\n",
      "  0.06238165 -0.16934377  0.00325949  0.17728482  0.03665325 -0.17558283\n",
      " -0.14631864  0.12002646 -0.26050505 -0.2420865   0.07576365  0.2746722\n",
      "  0.09456128 -0.0821784  -0.07241417 -0.14664602 -0.06794547 -0.22714663\n",
      "  0.17995358 -0.02002932 -0.14626063  0.07084303  0.09672233  0.16242114\n",
      " -0.15679023 -0.17761318  0.16524783  0.16408229 -0.1398159   0.08598263\n",
      "  0.1166006  -0.00138887  0.01759337  0.08898795 -0.00109552  0.00884543\n",
      " -0.10420994 -0.01112607 -0.05976019 -0.16435646  0.05247753 -0.14771546\n",
      " -0.08459281  0.13468072  0.13662091  0.00973886 -0.02802863  0.04633231\n",
      "  0.06382972  0.08765724 -0.05602548  0.04169456  0.10608517  0.12137013\n",
      "  0.09476804  0.07249533 -0.10288695  0.0228219   0.01481075  0.1044543\n",
      " -0.09729025  0.09143579 -0.06881186 -0.07392979  0.04186718  0.10864058\n",
      "  0.18302506  0.01768255  0.05375018  0.1553992   0.07217151 -0.05286982\n",
      " -0.05944532  0.02479465 -0.06331508 -0.04059208 -0.05508992 -0.15513411\n",
      " -0.0190439  -0.11533938  0.082515    0.07532124 -0.01641958 -0.01962966\n",
      "  0.09402367  0.01291552  0.17095733  0.08802138 -0.00640136  0.10692009\n",
      " -0.03596345 -0.1638008   0.15753913  0.04115119 -0.01593414  0.0846043\n",
      " -0.09602346  0.03788093  0.08638579  0.01365222  0.01116436 -0.00329547\n",
      "  0.03452149  0.06797067  0.09027591  0.12050607  0.10375713 -0.03440823\n",
      " -0.07749014 -0.08242656 -0.03343312  0.1823639  -0.08715254  0.16970326\n",
      " -0.08335125  0.02945024 -0.00125252  0.05370887  0.13504046 -0.06743855\n",
      "  0.19553334 -0.0953336   0.13459738  0.18605472  0.05541985 -0.12985498\n",
      " -0.10473096 -0.04593766  0.16045912 -0.05368752  0.07777667 -0.03974967\n",
      "  0.03014475 -0.09736861  0.08483238  0.0154598   0.0208945  -0.08722691\n",
      "  0.03570408 -0.08418008  0.09521382  0.2544435  -0.01322327 -0.06644795\n",
      "  0.16860098 -0.12442842  0.16352676  0.10353578 -0.04037392  0.15786122\n",
      "  0.03246357  0.02982097 -0.17515808 -0.01213384 -0.01141059  0.18792646\n",
      "  0.20909044 -0.1422515  -0.02998731  0.05757066  0.1444119  -0.04974873\n",
      " -0.12387629 -0.13470529  0.1990601  -0.19452481  0.18304434  0.07633557\n",
      "  0.09206077 -0.02875413  0.03283234  0.04524699  0.12549295  0.13701957\n",
      " -0.10029677  0.10428189 -0.20738599 -0.10536709  0.23718895 -0.07111749\n",
      " -0.23065828 -0.06363919 -0.13925736 -0.16621311 -0.1705271  -0.01166972\n",
      " -0.07360462  0.06995551  0.06799971 -0.12353949 -0.02553673  0.02261008\n",
      "  0.21973588  0.33468026 -0.19416872  0.06776997 -0.06656244  0.12340087\n",
      "  0.0040387  -0.15167631 -0.3088745   0.0177269  -0.0968862  -0.14135596\n",
      " -0.22125028 -0.14711343  0.11608397  0.04878295  0.11667771  0.12684935\n",
      "  0.09642615  0.07763374  0.09851693 -0.10791541  0.2277398  -0.134931\n",
      " -0.17928573  0.03785304 -0.25041917  0.18940581 -0.01400452  0.25846636\n",
      " -0.16989285 -0.0980546  -0.08824286 -0.16608791 -0.11689308 -0.03414302\n",
      " -0.15704557 -0.0382431   0.02795427  0.00803169 -0.12735762 -0.07972813\n",
      "  0.02994999 -0.1236677   0.08099119  0.1089789  -0.06905842  0.10017539\n",
      "  0.30556038  0.00530807 -0.02942093  0.17589234 -0.02127464 -0.04091316\n",
      " -0.05107918 -0.0380148   0.10778546 -0.19067715  0.1646507  -0.07765732\n",
      "  0.12159607 -0.18069665  0.05931366 -0.03585176  0.0319537   0.03471427\n",
      " -0.02730652 -0.02747949 -0.15766002 -0.08230922  0.13472696  0.01470818\n",
      " -0.05352124 -0.14907177 -0.04197985  0.03467455  0.12586558 -0.18937396\n",
      " -0.07210393 -0.09885684 -0.1949746  -0.02724035  0.11640505  0.15672438\n",
      " -0.14147054 -0.14518164  0.02903035  0.084234   -0.08458938  0.07799228\n",
      "  0.14946514  0.08995713 -0.02256267  0.1857213   0.06669686  0.00102142]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec parameters (see train_word2vec)\n",
    "embedding_dim = 300\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#Prepare embedding layer weights for not-static model\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                   min_word_count=min_word_count, context=context)\n",
    "\n",
    "print(embedding_weights[565]) # 565 is the index word rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, GlobalMaxPooling1D, Conv1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_base_model(vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob, hidden_dims, sequence_length):\n",
    "    # Input\n",
    "    input_shape = (sequence_length,)\n",
    "    input_layer = Input(shape=input_shape, name='input_layer')  # (?, 56)\n",
    "\n",
    "    # Embedding\n",
    "    embedded = Embedding(input_dim=vocab_size,\n",
    "                         output_dim=embedding_dim,\n",
    "                         input_length=sequence_length,\n",
    "                         name='embedding_layer')(input_layer) # (batch_size, sequence_length, output_dim)=(?, 56, 50),\n",
    "\n",
    "    # CNN, iterate filter_size\n",
    "    conv_blocks = []\n",
    "    for fz in filter_sizes:\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=fz,\n",
    "                      padding='valid',  # valid means no padding\n",
    "                      strides=1,\n",
    "                      activation='relu',\n",
    "                      use_bias=True)(embedded)\n",
    "        conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    concat1max = Concatenate()(conv_blocks)\n",
    "    concat1max = Dropout(dropout_prob[1])(concat1max) # 0.8\n",
    "    output_layer = Dense(hidden_dims, activation='relu',\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         bias_regularizer=regularizers.l1(0.01))(concat1max) # (?, 50)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer) # (?, 1)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 300)\n",
      "Train on 8635 samples, validate on 960 samples\n",
      "Epoch 1/10\n",
      " - 70s - loss: 1.2636 - acc: 0.5187 - val_loss: 0.9801 - val_acc: 0.5479\n",
      "Epoch 2/10\n",
      " - 58s - loss: 0.9001 - acc: 0.5180 - val_loss: 0.8283 - val_acc: 0.5052\n",
      "Epoch 3/10\n",
      " - 62s - loss: 0.7938 - acc: 0.5353 - val_loss: 0.7629 - val_acc: 0.5510\n",
      "Epoch 4/10\n",
      " - 68s - loss: 0.7444 - acc: 0.5421 - val_loss: 0.7292 - val_acc: 0.5615\n",
      "Epoch 5/10\n",
      " - 59s - loss: 0.7163 - acc: 0.5600 - val_loss: 0.7092 - val_acc: 0.5792\n",
      "Epoch 6/10\n",
      " - 65s - loss: 0.6806 - acc: 0.5942 - val_loss: 0.6820 - val_acc: 0.5969\n",
      "Epoch 7/10\n",
      " - 65s - loss: 0.5809 - acc: 0.7060 - val_loss: 0.5920 - val_acc: 0.7083\n",
      "Epoch 8/10\n",
      " - 58s - loss: 0.4106 - acc: 0.8225 - val_loss: 0.6186 - val_acc: 0.7156\n",
      "Epoch 9/10\n",
      " - 69s - loss: 0.3067 - acc: 0.8777 - val_loss: 0.6390 - val_acc: 0.7406\n",
      "Epoch 10/10\n",
      " - 62s - loss: 0.2217 - acc: 0.9170 - val_loss: 0.7577 - val_acc: 0.7448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12728e978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "vocab_size = len(vocabulary_inv)\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Create model\n",
    "sequence_length = x_test.shape[1]  # 56\n",
    "model = create_base_model(vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob, hidden_dims, sequence_length)\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train model with Early Stopping\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_split=0.1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067/1067 [==============================] - 1s 1ms/step\n",
      "[0.8646195792883719, 0.7019681348461019]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model from 'models/base_non_static_cnn.json'\n",
      "1067/1067 [==============================] - 2s 1ms/step\n",
      "[0.8646195792883719, 0.7019681348461019]\n"
     ]
    }
   ],
   "source": [
    "#================Save and Load=================\n",
    "from os.path import join, exists, split\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Save model\n",
    "model_dir = 'models'\n",
    "model_name = 'base_non_static_cnn.json'\n",
    "model_name = join(model_dir, model_name)\n",
    "model_weights = 'base_non_static_cnn.h5'\n",
    "model_weights = join(model_dir, model_weights)\n",
    "\n",
    "if not exists(model_name):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "    print('Saving non static cnn model and its in \\'%s\\'' % split(model_name)[0])\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(model_weights)\n",
    "else:\n",
    "    # Load json and create model\n",
    "    with open(model_name, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # Load weights into new model\n",
    "    loaded_model.load_weights(model_weights)\n",
    "    print('Loaded existing model from \\'%s\\'' % model_name)\n",
    "\n",
    "# Evaluate\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, y_test) # Must compile before evaluate\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the model, if we want to use `evaluate` function, we still have to compile the model. Here we just use `predcit` function, and use sklearn metrics to evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25923103],\n",
       "       [0.15802522],\n",
       "       [0.44756457],\n",
       "       ...,\n",
       "       [0.9516991 ],\n",
       "       [0.9999974 ],\n",
       "       [0.01045262]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(x_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25923103, 0.15802522, 0.44756457, ..., 0.9516991 , 0.9999974 ,\n",
       "       0.01045262], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = prediction.flatten()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.where(prediction > 0.5, 1, 0)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019681349578257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prediciton\n",
    "prediction = model.predict(x_test)\n",
    "prediction = prediction.flatten()\n",
    "prediction = np.where(prediction > 0.5, 1, 0)\n",
    "score = accuracy_score(y_test, prediction)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019681349578257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prediciton\n",
    "prediction = loaded_model.predict(x_test)\n",
    "prediction = prediction.flatten()\n",
    "prediction = np.where(prediction > 0.5, 1, 0)\n",
    "score = accuracy_score(y_test, prediction)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
