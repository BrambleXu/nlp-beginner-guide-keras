{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "If the the dimesion of y is 1d, e.g. `[1, 0]`, the first sample is positive and second sample is negative. The activate function in output layer should be sigmoid. If we use softmax, the loss will become very big and hard to converge. \n",
    "> (1) show the loss, softamx with one output neuron\n",
    "\n",
    "Becasue softmax will assing probability for each class, and the total sum of the probabilities over all classes equals to one. And the number of neuron in output layer is only 1, this will cause the output of softamx will all become 1. \n",
    "> (2) show softmax output, all equal to 1\n",
    "\n",
    "But in this situation, the sigmoid will work successfully.\n",
    "> (3) show the loss, sigmoid with one output neuron\n",
    "\n",
    "If the dimension of y is 2d, e.g. `[[0, 1], [1, 0]]`, `[0, 1]` means the positive sample, and `[1, 0]` means the negtive sample. In this way, the output neuron is 2, so we can use the softamx with normal loss. \n",
    "> (4) show the loss, softmax with two output neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from word2vec import train_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n",
      "x_train shape:  (9595, 56)\n",
      "x_test shape: (1067, 56)\n",
      "Vocabulary Size: 18765\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "y = y.argmax(axis=1) # y: [1, 1, 1, ...., 0, 0, 0]. 1 for positive, 0 for negative\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "\n",
    "# Output shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Vocabulary Size: {:d}'.format(len(vocabulary_inv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing Word2Vec model '50feature_1minwords_10context'\n",
      "[ 0.116417   -0.08654545 -0.2803645   0.3124824  -0.09874617 -0.01292455\n",
      "  0.5993319   0.37595585 -0.19778901  0.3446241  -0.267074   -0.46938565\n",
      "  0.06542085  0.13901834 -0.23808217 -0.26174366 -0.24359785 -0.2912121\n",
      " -0.31934163 -0.5253044   0.14535798  0.13424076 -0.34564495  0.11147276\n",
      " -0.39667356  0.153825    0.00460984 -0.05314974  0.03791095 -0.26470512\n",
      "  0.5289599   0.64035916  0.23924957 -0.11943628  0.16350754 -0.3181289\n",
      "  0.60478944  0.12693582 -0.014964    0.16417578  0.07708839  0.14353544\n",
      "  0.17967053  0.4206976   0.18995559 -0.51884794  0.17004833 -0.14022829\n",
      "  0.27810973  0.10528004]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec parameters (see train_word2vec)\n",
    "embedding_dim = 50\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#Prepare embedding layer weights for not-static model\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                   min_word_count=min_word_count, context=context)\n",
    "\n",
    "print(embedding_weights[565]) # 565 is the index word rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, GlobalMaxPooling1D, Conv1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for how to read set the pre-train weight for embedding layer, please see here: https://github.com/keras-team/keras/issues/853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 8.0930 - acc: 0.5010 - val_loss: 8.1630 - val_acc: 0.4911\n",
      "Epoch 2/10\n",
      " - 3s - loss: 7.9773 - acc: 0.5010 - val_loss: 8.1195 - val_acc: 0.4911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 7.9579 - acc: 0.5010 - val_loss: 8.1137 - val_acc: 0.4911\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.9556 - acc: 0.5010 - val_loss: 8.1132 - val_acc: 0.4911\n",
      "Epoch 5/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 6/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 7/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 8/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 9/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 10/10\n",
      " - 3s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a34125320>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 1: big loss while using softmax only have 1 output neuron \n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='softmax', name='softmax_output')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# (1) big loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 56, 50)       938250      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 54, 10)       1510        embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 49, 10)       4010        embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 10)           0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 10)           0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 20)           0           global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 20)           0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 50)           1050        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 1)            51          dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 944,871\n",
      "Trainable params: 944,871\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/keras_learning_phase:0' shape=() dtype=bool>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_softmax_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[-1].output])\n",
    "\n",
    "# output in test mode = 0\n",
    "# layer_output = get_3rd_layer_output([X, 0])[0]\n",
    "\n",
    "# output in train mode = 1\n",
    "layer_output = get_softmax_layer_output([x_train, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output # (2) the output of softmax when have only 1 output neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.9497 - acc: 0.5077 - val_loss: 0.8551 - val_acc: 0.5286\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.8154 - acc: 0.5032 - val_loss: 0.7804 - val_acc: 0.5164\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.7598 - acc: 0.5213 - val_loss: 0.7424 - val_acc: 0.5098\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.7294 - acc: 0.5300 - val_loss: 0.7215 - val_acc: 0.5370\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.7137 - acc: 0.5271 - val_loss: 0.7076 - val_acc: 0.5633\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.7023 - acc: 0.5406 - val_loss: 0.7004 - val_acc: 0.5698\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.6949 - acc: 0.5425 - val_loss: 0.6943 - val_acc: 0.5661\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.6856 - acc: 0.5545 - val_loss: 0.6888 - val_acc: 0.5933\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.6708 - acc: 0.5752 - val_loss: 0.6787 - val_acc: 0.6073\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.6533 - acc: 0.5936 - val_loss: 0.6670 - val_acc: 0.6504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a40a376a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 2: sigmoid with one output neuron\n",
    "\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# (3) show the loss, sigmoid with one output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output neuron is 2 for the softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "# y = y.argmax(axis=1) # comment this to make the dimension of y is 2\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.9405 - acc: 0.5098 - val_loss: 0.8364 - val_acc: 0.5201\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.8002 - acc: 0.5028 - val_loss: 0.7699 - val_acc: 0.4948\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.7509 - acc: 0.5084 - val_loss: 0.7351 - val_acc: 0.5417\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.7270 - acc: 0.5224 - val_loss: 0.7172 - val_acc: 0.5426\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.7133 - acc: 0.5255 - val_loss: 0.7091 - val_acc: 0.5455\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.7035 - acc: 0.5240 - val_loss: 0.7024 - val_acc: 0.5539\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.6953 - acc: 0.5411 - val_loss: 0.6960 - val_acc: 0.5698\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.6838 - acc: 0.5518 - val_loss: 0.6899 - val_acc: 0.5689\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.6698 - acc: 0.5704 - val_loss: 0.6821 - val_acc: 0.5792\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.6431 - acc: 0.5934 - val_loss: 0.6588 - val_acc: 0.6485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2027ce10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 3: Output neuron is 2 for the softmax\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(2, activation='softmax')(output_layer) # change 1 to 2 as the output neuron\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "#  (4) show the loss, softmax with two output neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
