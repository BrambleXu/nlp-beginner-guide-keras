{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build the CNN model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from word2vec import train_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n",
      "x_train shape:  (9595, 56)\n",
      "x_test shape: (1067, 56)\n",
      "Vocabulary Size: 18765\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "y = y.argmax(axis=1) # y: [1, 1, 1, ...., 0, 0, 0]. 1 for positive, 0 for negative\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "\n",
    "# Output shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Vocabulary Size: {:d}'.format(len(vocabulary_inv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing Word2Vec model '50feature_1minwords_10context'\n",
      "[-0.22072488  0.03841191  0.25244865 -0.19596232  0.5254891  -0.22822355\n",
      " -0.00765032 -0.21729529  0.32325193 -0.1354228   0.28161174 -0.14135455\n",
      "  0.25298622  0.10028931  0.13398536 -0.05369192 -0.08600403 -0.25493133\n",
      " -0.15806714  0.28666434  0.19685866  0.14603579  0.04521525 -0.4055126\n",
      " -0.3777436   0.29809853 -0.3177484  -0.12307277  0.20872054 -0.09028962\n",
      "  0.30230698  0.2604237   0.5757977   0.37168625 -0.56569725  0.30448192\n",
      " -0.08910146  0.2877515   0.3957461  -0.18291692 -0.4497671   0.38631678\n",
      "  0.59380317 -0.16212505 -0.33610743  0.38453627 -0.0516694   0.319073\n",
      "  0.02394754 -0.23467964]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec parameters (see train_word2vec)\n",
    "embedding_dim = 50\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#Prepare embedding layer weights for not-static model\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                   min_word_count=min_word_count, context=context)\n",
    "\n",
    "print(embedding_weights[565]) # 565 is the index word rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, GlobalMaxPooling1D, Conv1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model_input:0\", shape=(?, 56), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#=======================Build model=========================\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer')\n",
    "print(model_input) # see the input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_layer/GatherV2:0\", shape=(?, 56, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The CNN-Non-Static has embedding layer\n",
    "# Construct word embedding layer\n",
    "embedding_layer = Embedding(input_dim=len(vocabulary_inv), output_dim=embedding_dim,\n",
    "                      input_length=sequence_length, name='embedding_layer')(input_layer)\n",
    "print(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"drop_layer1_1/cond/Merge:0\", shape=(?, 56, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "drop_layer = Dropout(dropout_prob[0], name='drop_layer1')(embedding_layer)\n",
    "print(drop_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stride in this context means the step of the convolution operation. For example, if you do valid convolution of two sequences of length 10 and 6, in general you get an output of length 5 (10 -6 +1). It means that sequence 2 moves “step by step” along sequence 1, using a step size of 1 when doing convolution. But if you set the stride of convolution 2, the output would be of length 3 ((10–6) / 2 + 1), meaning that sequence 2 moves “step by step” along sequence 1, using a step size of 2.\n",
    "\n",
    "In the below code cell, we set the strides as 1, and the sequence length is 56. So after the conv, the new sequence length(new_step) should be 56-3+1=54.\n",
    "\n",
    "As for the MaxPooling1D, here is a good [explanation](https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi). pool_size is like the kernel_szie in Conv1D, we will choose biggest number in two words vector. Because we set strides as 1, so the shape after MaxPooling1D is 54-2+1=53. If we set pool_size=2, strides=2, the shape after MaxPooling1D is 54/2=27. Because we see two words one times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_24/Relu:0\", shape=(?, 54, 10), dtype=float32)\n",
      "Tensor(\"max_pooling1d_23/Squeeze:0\", shape=(?, 27, 10), dtype=float32)\n",
      "Tensor(\"flatten_10/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# CNN, first we set filter_szies as 3, to see the output \n",
    "conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=3, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "print(conv) # output (batch_size, new_steps, filters)\n",
    "\n",
    "# Max pooling \n",
    "conv = MaxPooling1D(pool_size=2)(conv)\n",
    "print(conv) # (batch_size, downsampled_steps, features)\n",
    "\n",
    "# Flatten \n",
    "conv = Flatten()(conv)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "    conv = MaxPooling1D(pool_size=2, strides=1)(conv)\n",
    "#     conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'max_pooling1d_24/Squeeze:0' shape=(?, 53, 10) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling1d_25/Squeeze:0' shape=(?, 52, 10) dtype=float32>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxPooling1D is not the 1max pooling in the original paper, this might confuse the reader. So here we choose `GlobalMaxPooling1D()` to implement the 1-max pooling. We can see that after the conv, the output shape is (?, 54, 10), here 10 is the filter number, we also take it as the features. In the code cell below, I set `the num_filters=10`. We want to select one biggest number in each filers, so `GlobalMaxPooling1D ` will select the biggest number on the `axis=1`(column), then we get a result of column vector with size 10. This is the `#1max` in the below image. \n",
    "\n",
    "![](http://www.joshuakim.io/wp-content/uploads/2017/12/figure.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_26/Relu:0\", shape=(?, 54, 10), dtype=float32)\n",
      "Tensor(\"global_max_pooling1d_1/Max:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D, this is the 1-max pooling in the paper\n",
    "# CNN, first we set filter_szies as 3, to see the output \n",
    "conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=3, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "print(conv) # output (batch_size, new_steps, filters)\n",
    "\n",
    "# Max pooling \n",
    "conv = GlobalMaxPooling1D()(conv) # this is equal to the #1max\n",
    "print(conv) # (batch_size, a max feature in a filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'global_max_pooling1d_8/Max:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'global_max_pooling1d_9/Max:0' shape=(?, 10) dtype=float32>]\n",
      "\n",
      "Tensor(\"concatenate_5/concat:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D, this is the 1-max pooling in the paper\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "print(conv_blocks)\n",
    "concat1max = Concatenate()(conv_blocks)\n",
    "print()\n",
    "print(concat1max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output_layer/Softmax:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer = Dense(1, activation='softmax', name='output_layer')(concat1max)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.88650054 -2.160063    0.96392006 -0.26911142  2.4589605  -0.854224\n",
      " -0.10222454 -0.9868089   0.49081135 -1.3237724   0.4743087  -0.07651804\n",
      " -0.04362829  0.01618732 -0.5368535  -0.53789335 -0.4597094  -0.867225\n",
      " -0.07294422  1.3061765   1.454337    0.50052536  0.02490961 -1.0968294\n",
      " -0.90007854  0.8476956  -0.51648337 -0.4541734   0.5217278  -0.38841093\n",
      "  1.1157492   1.6498995   2.7924497   2.4142728  -1.6991649   0.07605273\n",
      "  0.12023915  0.389353    1.4011778  -0.36487138 -2.1056702   0.6316661\n",
      "  2.2205415  -0.25847605 -0.6343487   0.84552944  0.05227763  1.5032666\n",
      "  0.12233521 -0.42744634]\n",
      "1 [-2.4797058  -0.65706545 -0.34030366 -0.88477564  1.9897318  -0.4754672\n",
      "  0.5620251  -1.7522604   1.7267761  -0.04009154  0.9425473  -0.33500427\n",
      "  1.0954219  -0.5345138   0.07658719 -1.1215198  -1.3558666  -1.5171361\n",
      " -0.54756194  3.0494883  -0.12976724  0.6956076   0.7036669  -2.0121005\n",
      " -0.6086946   2.0486102  -0.9583363  -1.5337933   2.692299   -0.12512176\n",
      "  1.5774658   0.5596391   3.48457     1.2297804  -2.371431    1.3041342\n",
      " -0.0529931   0.76171404  1.958666   -1.38912    -3.028361    1.5141562\n",
      "  2.371581   -2.2459679  -2.0873232   1.6380153  -1.5446606   3.4610384\n",
      " -1.2612183  -0.86829257]\n",
      "2 [-1.4178641  -0.03496985  1.9243873  -0.12215576  3.6067514  -0.46756825\n",
      " -0.1834845  -2.2934728   1.9467343  -0.12264703  2.0142872   0.897062\n",
      "  0.49687737  1.4114573   0.9952766  -0.63899654 -1.1299584  -1.253377\n",
      " -0.57058895  1.9490939   0.62293893 -0.11754252  0.5210756  -1.376098\n",
      " -1.9573841   2.0217319  -1.8940781  -0.13929592  1.1061765  -0.13431294\n",
      "  1.6127254   1.5822889   2.781428    1.3910265  -2.8188443   1.3353944\n",
      " -0.08893378  2.3408296   1.672834   -1.7171463  -3.0842      1.1817131\n",
      "  1.9799179  -0.79634935 -1.4596      1.9001875   0.36698145  1.3689892\n",
      " -0.22205094 -1.611271  ]\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(embedding_weights.items()):\n",
    "    if i < 3:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([v for v in embedding_weights.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18765, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.88650054, -2.160063  ,  0.96392006, -0.26911142,  2.4589605 ,\n",
       "       -0.854224  , -0.10222454, -0.9868089 ,  0.49081135, -1.3237724 ,\n",
       "        0.4743087 , -0.07651804, -0.04362829,  0.01618732, -0.5368535 ,\n",
       "       -0.53789335, -0.4597094 , -0.867225  , -0.07294422,  1.3061765 ,\n",
       "        1.454337  ,  0.50052536,  0.02490961, -1.0968294 , -0.90007854,\n",
       "        0.8476956 , -0.51648337, -0.4541734 ,  0.5217278 , -0.38841093,\n",
       "        1.1157492 ,  1.6498995 ,  2.7924497 ,  2.4142728 , -1.6991649 ,\n",
       "        0.07605273,  0.12023915,  0.389353  ,  1.4011778 , -0.36487138,\n",
       "       -2.1056702 ,  0.6316661 ,  2.2205415 , -0.25847605, -0.6343487 ,\n",
       "        0.84552944,  0.05227763,  1.5032666 ,  0.12233521, -0.42744634],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for how to read set the pre-train weight for embedding layer, please see here: https://github.com/keras-team/keras/issues/853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in one cell \n",
    "# version 1, not converge\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "weights = np.array([v for v in embedding_weights.values()]) # assemble the embedding_weights in one numpy array\n",
    "embedding_layer = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            trainable=True, # set True so embedding weight will be updated during training\n",
    "                            name='embedding_layer')\n",
    "\n",
    "embedding_layer.build((None,)) # if you don't do this, the next step won't work\n",
    "embedding_layer.set_weights([weights]) # use pre-trained word vector as the weights\n",
    "\n",
    "embedded = embedding_layer(input_layer)                  \n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 2/10\n",
      " - 4s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 3/10\n",
      " - 5s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 4/10\n",
      " - 5s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 5/10\n",
      " - 5s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 6/10\n",
      " - 5s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 7/10\n",
      " - 6s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 8/10\n",
      " - 6s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 9/10\n",
      " - 6s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n",
      "Epoch 10/10\n",
      " - 6s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134119e48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/5\n",
      " - 5s - loss: 8.1122 - acc: 0.5010 - val_loss: 8.1726 - val_acc: 0.4911\n",
      "Epoch 2/5\n",
      " - 5s - loss: 7.9820 - acc: 0.5010 - val_loss: 8.1211 - val_acc: 0.4911\n",
      "Epoch 3/5\n",
      " - 5s - loss: 7.9585 - acc: 0.5010 - val_loss: 8.1138 - val_acc: 0.4911\n",
      "Epoch 4/5\n",
      " - 5s - loss: 7.9556 - acc: 0.5010 - val_loss: 8.1132 - val_acc: 0.4911\n",
      "Epoch 5/5\n",
      " - 6s - loss: 7.9554 - acc: 0.5010 - val_loss: 8.1131 - val_acc: 0.4911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127309cc0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 2， change embedding layer\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.9279 - acc: 0.5071 - val_loss: 0.8481 - val_acc: 0.5108\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.8112 - acc: 0.5221 - val_loss: 0.7803 - val_acc: 0.5389\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.7633 - acc: 0.5205 - val_loss: 0.7472 - val_acc: 0.5408\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.7341 - acc: 0.5379 - val_loss: 0.7273 - val_acc: 0.5314\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.7175 - acc: 0.5364 - val_loss: 0.7144 - val_acc: 0.5239\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.7062 - acc: 0.5385 - val_loss: 0.7055 - val_acc: 0.5483\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.6958 - acc: 0.5481 - val_loss: 0.6979 - val_acc: 0.5679\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.6877 - acc: 0.5502 - val_loss: 0.6902 - val_acc: 0.5679\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.6714 - acc: 0.5690 - val_loss: 0.6849 - val_acc: 0.6204\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.6535 - acc: 0.5949 - val_loss: 0.6752 - val_acc: 0.6317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132e3f780>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 3， activation from softmax to sigmoid\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.7236 - acc: 0.5067 - val_loss: 0.6941 - val_acc: 0.4902\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6937 - acc: 0.5062 - val_loss: 0.6937 - val_acc: 0.4799\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.6935 - acc: 0.5092 - val_loss: 0.6941 - val_acc: 0.4911\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.6921 - acc: 0.5109 - val_loss: 0.6928 - val_acc: 0.5023\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.6905 - acc: 0.5236 - val_loss: 0.6926 - val_acc: 0.5155\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.6899 - acc: 0.5179 - val_loss: 0.6914 - val_acc: 0.5417\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.6865 - acc: 0.5206 - val_loss: 0.6908 - val_acc: 0.5201\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.6791 - acc: 0.5281 - val_loss: 0.6885 - val_acc: 0.5586\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.6717 - acc: 0.5467 - val_loss: 0.6812 - val_acc: 0.5661\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.6604 - acc: 0.5619 - val_loss: 0.6767 - val_acc: 0.5754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139e38860>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 4, no l2 norm\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "concat1max = Dense(hidden_dims, activation='relu')(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(concat1max)\n",
    "\n",
    "# output_layer = Dense(hidden_dims, activation=\"relu\")(concat1max)\n",
    "# output_layer = Dense(1, activation=\"sigmoid\")(output_layer)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max_pooling1d_8/Squeeze:0\", shape=(?, 27, 10), dtype=float32)\n",
      "Tensor(\"flatten_8/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"max_pooling1d_9/Squeeze:0\", shape=(?, 24, 10), dtype=float32)\n",
      "Tensor(\"flatten_9/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"concatenate_22/concat:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(embedded)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    print(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    print(conv)\n",
    "    conv_blocks.append(conv)\n",
    "    \n",
    "concat1max = Concatenate()(conv_blocks) # (?, 20)\n",
    "print(concat1max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When kernel_szie=3, the max_pooling size is `(?, 27, 10)`, means each filter extrac 27 biggest number. After flatten, the size is `(?, 270)`. \n",
    "\n",
    "When kernel_szie=8, the max_pooling size is `(?, 24, 10)`, means each filter extrac 24 biggest number. After flatten, the size is `(?, 240)`. \n",
    "\n",
    "Finally, after the concatenate, the shape is `(?, 510)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_23/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 1.1724 - acc: 0.5052 - val_loss: 0.8976 - val_acc: 0.5220\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.8191 - acc: 0.5177 - val_loss: 0.7665 - val_acc: 0.5098\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.7398 - acc: 0.5317 - val_loss: 0.7245 - val_acc: 0.5426\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.7110 - acc: 0.5379 - val_loss: 0.7047 - val_acc: 0.5633\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.6969 - acc: 0.5556 - val_loss: 0.6979 - val_acc: 0.5754\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.6910 - acc: 0.5659 - val_loss: 0.6849 - val_acc: 0.5886\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.6730 - acc: 0.5985 - val_loss: 0.6689 - val_acc: 0.6186\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.6515 - acc: 0.6246 - val_loss: 0.6285 - val_acc: 0.6954\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.5951 - acc: 0.6847 - val_loss: 0.6014 - val_acc: 0.6992\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.5307 - acc: 0.7535 - val_loss: 0.5790 - val_acc: 0.7151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bf53ba8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 5，global max pooling to max 1d pooling, pooling size=2\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                  kernel_regularizer=regularizers.l2(0.01),\n",
    "                  bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_24/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.7533 - acc: 0.5085 - val_loss: 0.6919 - val_acc: 0.5576\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.6924 - acc: 0.5288 - val_loss: 0.6926 - val_acc: 0.5436\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.6904 - acc: 0.5268 - val_loss: 0.6880 - val_acc: 0.5548\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.6876 - acc: 0.5424 - val_loss: 0.6870 - val_acc: 0.5708\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.6856 - acc: 0.5453 - val_loss: 0.6865 - val_acc: 0.5614\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.6817 - acc: 0.5534 - val_loss: 0.6793 - val_acc: 0.5858\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.6660 - acc: 0.5847 - val_loss: 0.6692 - val_acc: 0.6148\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.6441 - acc: 0.6217 - val_loss: 0.6385 - val_acc: 0.6776\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.6078 - acc: 0.6551 - val_loss: 0.6017 - val_acc: 0.6795\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.5663 - acc: 0.6923 - val_loss: 0.5672 - val_acc: 0.7113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ac5d588>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 6，no l2 reg\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu')(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_26/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.6962 - acc: 0.5290 - val_loss: 0.6971 - val_acc: 0.5305\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.6691 - acc: 0.5895 - val_loss: 0.6867 - val_acc: 0.5726\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.6294 - acc: 0.6502 - val_loss: 0.6720 - val_acc: 0.5970\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.4882 - acc: 0.7713 - val_loss: 0.6478 - val_acc: 0.6767\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.2457 - acc: 0.9030 - val_loss: 0.5829 - val_acc: 0.7320\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.1126 - acc: 0.9610 - val_loss: 0.6681 - val_acc: 0.7310\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.0568 - acc: 0.9821 - val_loss: 0.7781 - val_acc: 0.7488\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0269 - acc: 0.9937 - val_loss: 0.8768 - val_acc: 0.7376\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.0125 - acc: 0.9978 - val_loss: 0.9962 - val_acc: 0.7423\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0062 - acc: 0.9993 - val_loss: 1.1184 - val_acc: 0.7395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14028ce80>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 7，add/delete dropout\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "# # Dropout\n",
    "# embedded = Dropout(dropout_prob[0])(embedded)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "# concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu')(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_27/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.7376 - acc: 0.5087 - val_loss: 0.7040 - val_acc: 0.5258\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.7024 - acc: 0.5201 - val_loss: 0.7023 - val_acc: 0.5080\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.6989 - acc: 0.5263 - val_loss: 0.6971 - val_acc: 0.5380\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.6946 - acc: 0.5423 - val_loss: 0.6945 - val_acc: 0.5576\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.6901 - acc: 0.5432 - val_loss: 0.6899 - val_acc: 0.5567\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.6858 - acc: 0.5585 - val_loss: 0.6872 - val_acc: 0.5783\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.6792 - acc: 0.5772 - val_loss: 0.6820 - val_acc: 0.5839\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.6668 - acc: 0.5991 - val_loss: 0.6723 - val_acc: 0.5923\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.6419 - acc: 0.6312 - val_loss: 0.6452 - val_acc: 0.6354\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.5720 - acc: 0.7004 - val_loss: 0.5748 - val_acc: 0.7113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13fbaeef0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 8，set l2 reg in the final sigmoid layer\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu')(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid', \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     bias_regularizer=regularizers.l1(0.01))(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_34/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/100\n",
      " - 20s - loss: 1.0627 - acc: 0.5007 - val_loss: 0.8224 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      " - 17s - loss: 0.7809 - acc: 0.5019 - val_loss: 0.7574 - val_acc: 0.5108\n",
      "Epoch 3/100\n",
      " - 16s - loss: 0.7579 - acc: 0.5349 - val_loss: 0.7626 - val_acc: 0.5351\n",
      "Epoch 4/100\n",
      " - 20s - loss: 0.7507 - acc: 0.5472 - val_loss: 0.7591 - val_acc: 0.5633\n",
      "Epoch 5/100\n",
      " - 17s - loss: 0.7356 - acc: 0.5851 - val_loss: 0.7399 - val_acc: 0.5951\n",
      "Epoch 6/100\n",
      " - 17s - loss: 0.7168 - acc: 0.6222 - val_loss: 0.7294 - val_acc: 0.6317\n",
      "Epoch 7/100\n",
      " - 17s - loss: 0.6553 - acc: 0.7149 - val_loss: 0.6599 - val_acc: 0.7142\n",
      "Epoch 8/100\n",
      " - 19s - loss: 0.4602 - acc: 0.8400 - val_loss: 0.6213 - val_acc: 0.7413\n",
      "Epoch 9/100\n",
      " - 16s - loss: 0.3230 - acc: 0.8996 - val_loss: 0.6494 - val_acc: 0.7432\n",
      "Epoch 10/100\n",
      " - 17s - loss: 0.2418 - acc: 0.9302 - val_loss: 0.6748 - val_acc: 0.7348\n",
      "Epoch 11/100\n",
      " - 20s - loss: 0.2108 - acc: 0.9416 - val_loss: 0.7129 - val_acc: 0.7357\n",
      "Epoch 12/100\n",
      " - 17s - loss: 0.1721 - acc: 0.9550 - val_loss: 0.8372 - val_acc: 0.7385\n",
      "Epoch 13/100\n",
      " - 18s - loss: 0.1591 - acc: 0.9597 - val_loss: 0.8833 - val_acc: 0.7320\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1154742e8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 9，filter_sizes = (3, 4, 5), num_filters = 100\n",
    "#=======================Build model=========================\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(embedded) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=[earlystopper],\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_35/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Initializing embedding layer with word2vec weights, shape (18765, 50)\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/100\n",
      " - 9s - loss: 1.4122 - acc: 0.5034 - val_loss: 1.1174 - val_acc: 0.5098\n",
      "Epoch 2/100\n",
      " - 7s - loss: 1.0165 - acc: 0.5086 - val_loss: 0.9147 - val_acc: 0.5248\n",
      "Epoch 3/100\n",
      " - 7s - loss: 0.8606 - acc: 0.5229 - val_loss: 0.8109 - val_acc: 0.5342\n",
      "Epoch 4/100\n",
      " - 7s - loss: 0.7822 - acc: 0.5355 - val_loss: 0.7583 - val_acc: 0.5314\n",
      "Epoch 5/100\n",
      " - 7s - loss: 0.7412 - acc: 0.5472 - val_loss: 0.7238 - val_acc: 0.5820\n",
      "Epoch 6/100\n",
      " - 7s - loss: 0.7181 - acc: 0.5646 - val_loss: 0.7121 - val_acc: 0.5736\n",
      "Epoch 7/100\n",
      " - 8s - loss: 0.7061 - acc: 0.5696 - val_loss: 0.7109 - val_acc: 0.5361\n",
      "Epoch 8/100\n",
      " - 8s - loss: 0.6915 - acc: 0.5919 - val_loss: 0.6864 - val_acc: 0.5989\n",
      "Epoch 9/100\n",
      " - 7s - loss: 0.6786 - acc: 0.6097 - val_loss: 0.6799 - val_acc: 0.6373\n",
      "Epoch 10/100\n",
      " - 7s - loss: 0.6344 - acc: 0.6628 - val_loss: 0.6665 - val_acc: 0.5670\n",
      "Epoch 11/100\n",
      " - 7s - loss: 0.5485 - acc: 0.7358 - val_loss: 0.8724 - val_acc: 0.5548\n",
      "Epoch 12/100\n",
      " - 8s - loss: 0.4277 - acc: 0.8213 - val_loss: 0.5644 - val_acc: 0.7263\n",
      "Epoch 13/100\n",
      " - 8s - loss: 0.3497 - acc: 0.8625 - val_loss: 0.6228 - val_acc: 0.7085\n",
      "Epoch 14/100\n",
      " - 7s - loss: 0.2983 - acc: 0.8844 - val_loss: 0.6777 - val_acc: 0.7057\n",
      "Epoch 15/100\n",
      " - 8s - loss: 0.2653 - acc: 0.9042 - val_loss: 0.5864 - val_acc: 0.7488\n",
      "Epoch 16/100\n",
      " - 8s - loss: 0.2340 - acc: 0.9140 - val_loss: 0.6209 - val_acc: 0.7535\n",
      "Epoch 17/100\n",
      " - 8s - loss: 0.2073 - acc: 0.9278 - val_loss: 0.6674 - val_acc: 0.7441\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144d7dda0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write in one cell \n",
    "# version 10，BatchNormalization\n",
    "#=======================Build model=========================\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "\n",
    "# Embedding \n",
    "embedded = Embedding(input_dim=len(vocabulary_inv), \n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=sequence_length, \n",
    "                            name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  use_bias=True)(embedded)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (?, 510)\n",
    "print(concat1max)\n",
    "concat1max = Dropout(dropout_prob[1])(concat1max)\n",
    "output_layer = Dense(hidden_dims, activation='relu', \n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     bias_regularizer=regularizers.l1(0.01))(concat1max)\n",
    "output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "weights = np.array([v for v in embedding_weights.values()])\n",
    "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "embedding_layer = model.get_layer(\"embedding_layer\")\n",
    "embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=[earlystopper],\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
