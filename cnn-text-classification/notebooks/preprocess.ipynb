{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess\n",
    "\n",
    "## 1 Load data\n",
    "\n",
    "In the `train.py`, we use one line to get the preprocessed data. \n",
    "\n",
    "`x_text, y = data_helpers.load_data_and_labels(positive_data_file_path, negative_data_file_path)`\n",
    "\n",
    "In the `data_helper.py`, we use `load_data_and_labels` and `clean_str` to preprocess the data. In this notebook, I will show want happend in this preprocess. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_str()` is used to process the special characters, we take it from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will to see what happend in the `load_data_and_labels()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.1 Load data from files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path parameter\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "# If we use python 3, we should set the encoding as utf-8, otherwise there will be an error. \n",
    "positive_examples = list(open(positive_data_file, 'r', encoding='utf-8').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \\n',\n",
       " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . \\n',\n",
       " 'effective but too-tepid biopic\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_examples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence looks bad,  we use `strip()` to delete the `\\n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = [s.strip() for s in positive_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       " 'effective but too-tepid biopic']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_examples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing with negative data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_examples = list(open(negtive_data_file, 'r', encoding='utf-8').readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"as it stands , crocodile hunter has the hurried , badly cobbled look of the 1959 godzilla , which combined scenes of a japanese monster flick with canned shots of raymond burr commenting on the monster's path of destruction .\",\n",
       " 'the thing looks like a made-for-home-video quickie .',\n",
       " \"enigma is well-made , but it's just too dry and too placid .\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_examples[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Split by words\n",
    "\n",
    "Here we will use `clean_str()` to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive sentence number:  5331\n",
      "The negative sentence number:  5331\n"
     ]
    }
   ],
   "source": [
    "# See the example number\n",
    "print('The positive sentence number: ', len(positive_examples))\n",
    "print('The negative sentence number: ', len(negative_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole sentence number:  10662\n"
     ]
    }
   ],
   "source": [
    "x_text = positive_examples + negative_examples\n",
    "print('The whole sentence number: ', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       " 'effective but too-tepid biopic']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 sentences are positive \n",
    "x_text[:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"as it stands , crocodile hunter has the hurried , badly cobbled look of the 1959 godzilla , which combined scenes of a japanese monster flick with canned shots of raymond burr commenting on the monster's path of destruction .\",\n",
       " 'the thing looks like a made-for-home-video quickie .',\n",
       " \"enigma is well-made , but it's just too dry and too placid .\"]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 3 sentences are negative\n",
    "x_text[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use clean_str() to process each sentence\n",
    "x_text = [clean_str(sent) for sent in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       " 'effective but too-tepid biopic']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"as it stands , crocodile hunter has the hurried , badly cobbled look of the 1959 godzilla , which combined scenes of a japanese monster flick with canned shots of raymond burr commenting on the monster's path of destruction .\",\n",
       " 'the thing looks like a made-for-home-video quickie .',\n",
       " \"enigma is well-made , but it's just too dry and too placid .\"]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each sentence to a list of words\n",
    "x_text = [sent.split(\" \") for sent in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'conan',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'arnold',\n",
       " 'schwarzenegger',\n",
       " ',',\n",
       " 'jean',\n",
       " 'claud',\n",
       " 'van',\n",
       " 'damme',\n",
       " 'or',\n",
       " 'steven',\n",
       " 'segal']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Generate labels\n",
    "\n",
    "For each sentence, using `[neg, pos]` to represent the lables.\n",
    "- If we have a positive label, we represent it as `[0, 1]`\n",
    "- If we have a negative label, we represent it as `[1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "# concatenate them together\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the positive example amd negative example are ordered, so we need to shuffle them when training to make the model learn better parameter. We will do this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pad\n",
    "\n",
    "Pad all sentences to the same length. We choose longest sentence as the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 38, 5, 20, 22]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(sent) for sent in x_text][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = max(len(sent) for sent in x_text)\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_word=\"<PAD/>\"\n",
    "\n",
    "for i in range(len(x_text)):\n",
    "    sentence = x_text[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [padding_word] * num_padding\n",
    "    padded_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " \"'s\",\n",
       " 'new',\n",
       " 'conan',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'arnold',\n",
       " 'schwarzenegger',\n",
       " ',',\n",
       " 'jean',\n",
       " 'claud',\n",
       " 'van',\n",
       " 'damme',\n",
       " 'or',\n",
       " 'steven',\n",
       " 'segal',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>',\n",
       " '<PAD/>']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Build vocabulary \n",
    "\n",
    "Here we use the whole sentences to build vocabulary. There is one thing we should pay attention that in a normal nlp project, we only use the training data to build vocabulary. And when we process the testing data, there are some words that do not appear in the vocabulary. This is known as **oov** problem. \n",
    "\n",
    "> out of vocabulary(oov): used in computational linguistics and natural language processing for terms encountered in input which are not present in a system's dictionary or database of known terms\n",
    "\n",
    "Because this is the first project, we use the whole data set to build the vocabulary for easy understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "word_counts = Counter(itertools.chain(*padded_sentences)) # word_counts = {'the': 10194', 'rock': 39, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD/>', 379718),\n",
       " ('the', 10194),\n",
       " (',', 10037),\n",
       " ('a', 7341),\n",
       " ('and', 6264),\n",
       " ('of', 6148),\n",
       " ('to', 4275),\n",
       " ('is', 3562),\n",
       " (\"'s\", 3544),\n",
       " ('it', 3428)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD/>', 'the', ',', 'a', 'and', 'of', 'to', 'is', \"'s\", 'it']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_inv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<PAD/>', 0)\n",
      "('the', 1)\n",
      "(',', 2)\n",
      "('a', 3)\n",
      "('and', 4)\n",
      "('of', 5)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocabulary.items()):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Map sentences and labels to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 56)\n",
      "[    1   565     7  2633     6    22     1  3369   887     8   100  5598\n",
      "     4    11    65     8   240     6    73     3  3913    57  2948    34\n",
      "  1489  2393     2  2394 10111  1708  7197    42   937 10112     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first to get a dict version of vocabulary inverse\n",
    "vocabulary_inv = {value: key for key, value in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '<PAD/>')\n",
      "(1, 'the')\n",
      "(2, ',')\n",
      "(3, 'a')\n",
      "(4, 'and')\n",
      "(5, 'of')\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocabulary_inv.items()):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y[:5]) # 1 means positive\n",
    "print(y[-5:]) # 0 means negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle setting\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7359,  5573, 10180, ...,  1344,  7293,  1289])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(y))) # len(y): 10662\n",
    "shuffle_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Split train/test set\n",
    "\n",
    "back to train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rate = 0.9\n",
    "train_len = int(len(x) * 0.9)\n",
    "\n",
    "x_train = x[:train_len]\n",
    "y_train = y[:train_len]\n",
    "x_test = x[train_len:]\n",
    "y_test = y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
