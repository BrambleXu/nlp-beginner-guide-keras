{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build the CNN model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from word2vec import train_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "The sequence length is:  56\n",
      "x_train shape:  (9595, 56)\n",
      "x_test shape: (1067, 56)\n",
      "Vocabulary Size: 18765\n"
     ]
    }
   ],
   "source": [
    "# preprocess \n",
    "\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n",
    "\n",
    "# Pad sentence\n",
    "print(\"Padding sentences...\")\n",
    "x_text = data_helpers.pad_sentences(x_text)\n",
    "print(\"The sequence length is: \", len(x_text[0]))\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary, vocabulary_inv = data_helpers.build_vocab(x_text)\n",
    "\n",
    "# Represent sentence with word index, using word index to represent a sentence\n",
    "x = data_helpers.build_index_sentence(x_text, vocabulary)\n",
    "y = y.argmax(axis=1) # y: [1, 1, 1, ...., 0, 0, 0]. 1 for positive, 0 for negative\n",
    "\n",
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "\n",
    "# Output shape\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Vocabulary Size: {:d}'.format(len(vocabulary_inv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Word2Vec model '50feature_1minwords_10context'\n",
      "[-0.22072488  0.03841191  0.25244865 -0.19596232  0.5254891  -0.22822355\n",
      " -0.00765032 -0.21729529  0.32325193 -0.1354228   0.28161174 -0.14135455\n",
      "  0.25298622  0.10028931  0.13398536 -0.05369192 -0.08600403 -0.25493133\n",
      " -0.15806714  0.28666434  0.19685866  0.14603579  0.04521525 -0.4055126\n",
      " -0.3777436   0.29809853 -0.3177484  -0.12307277  0.20872054 -0.09028962\n",
      "  0.30230698  0.2604237   0.5757977   0.37168625 -0.56569725  0.30448192\n",
      " -0.08910146  0.2877515   0.3957461  -0.18291692 -0.4497671   0.38631678\n",
      "  0.59380317 -0.16212505 -0.33610743  0.38453627 -0.0516694   0.319073\n",
      "  0.02394754 -0.23467964]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec parameters (see train_word2vec)\n",
    "embedding_dim = 50\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#Prepare embedding layer weights for not-static model\n",
    "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                   min_word_count=min_word_count, context=context)\n",
    "\n",
    "print(embedding_weights[565]) # 565 is the index word rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, GlobalMaxPooling1D, Conv1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model_input:0\", shape=(?, 56), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#=======================Build model=========================\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer')\n",
    "print(model_input) # see the input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_layer/GatherV2:0\", shape=(?, 56, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The CNN-Non-Static has embedding layer\n",
    "# Construct word embedding layer\n",
    "embedding_layer = Embedding(input_dim=len(vocabulary_inv), output_dim=embedding_dim,\n",
    "                      input_length=sequence_length, name='embedding_layer')(input_layer)\n",
    "print(embedding_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"drop_layer1_1/cond/Merge:0\", shape=(?, 56, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "drop_layer = Dropout(dropout_prob[0], name='drop_layer1')(embedding_layer)\n",
    "print(drop_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stride in this context means the step of the convolution operation. For example, if you do valid convolution of two sequences of length 10 and 6, in general you get an output of length 5 (10 -6 +1). It means that sequence 2 moves “step by step” along sequence 1, using a step size of 1 when doing convolution. But if you set the stride of convolution 2, the output would be of length 3 ((10–6) / 2 + 1), meaning that sequence 2 moves “step by step” along sequence 1, using a step size of 2.\n",
    "\n",
    "In the below code cell, we set the strides as 1, and the sequence length is 56. So after the conv, the new sequence length(new_step) should be 56-3+1=54.\n",
    "\n",
    "As for the MaxPooling1D, here is a good [explanation](https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi). pool_size is like the kernel_szie in Conv1D, we will choose biggest number in two words vector. Because we set strides as 1, so the shape after MaxPooling1D is 54-2+1=53. If we set pool_size=2, strides=2, the shape after MaxPooling1D is 54/2=27. Because we see two words one times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_24/Relu:0\", shape=(?, 54, 10), dtype=float32)\n",
      "Tensor(\"max_pooling1d_23/Squeeze:0\", shape=(?, 27, 10), dtype=float32)\n",
      "Tensor(\"flatten_10/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# CNN, first we set filter_szies as 3, to see the output \n",
    "conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=3, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "print(conv) # output (batch_size, new_steps, filters)\n",
    "\n",
    "# Max pooling \n",
    "conv = MaxPooling1D(pool_size=2)(conv)\n",
    "print(conv) # (batch_size, downsampled_steps, features)\n",
    "\n",
    "# Flatten \n",
    "conv = Flatten()(conv)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "    conv = MaxPooling1D(pool_size=2, strides=1)(conv)\n",
    "#     conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'max_pooling1d_24/Squeeze:0' shape=(?, 53, 10) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling1d_25/Squeeze:0' shape=(?, 52, 10) dtype=float32>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxPooling1D is not the 1max pooling in the original paper, this might confuse the reader. So here we choose `GlobalMaxPooling1D()` to implement the 1-max pooling. We can see that after the conv, the output shape is (?, 54, 10), here 10 is the filter number, we also take it as the features. In the code cell below, I set `the num_filters=10`. We want to select one biggest number in each filers, so `GlobalMaxPooling1D ` will select the biggest number on the `axis=1`(column), then we get a result of column vector with size 10. This is the `#1max` in the below image. \n",
    "\n",
    "![](http://www.joshuakim.io/wp-content/uploads/2017/12/figure.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d_26/Relu:0\", shape=(?, 54, 10), dtype=float32)\n",
      "Tensor(\"global_max_pooling1d_1/Max:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D, this is the 1-max pooling in the paper\n",
    "# CNN, first we set filter_szies as 3, to see the output \n",
    "conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=3, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "print(conv) # output (batch_size, new_steps, filters)\n",
    "\n",
    "# Max pooling \n",
    "conv = GlobalMaxPooling1D()(conv) # this is equal to the #1max\n",
    "print(conv) # (batch_size, a max feature in a filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'global_max_pooling1d_8/Max:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'global_max_pooling1d_9/Max:0' shape=(?, 10) dtype=float32>]\n",
      "\n",
      "Tensor(\"concatenate_5/concat:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D, this is the 1-max pooling in the paper\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "print(conv_blocks)\n",
    "concat1max = Concatenate()(conv_blocks)\n",
    "print()\n",
    "print(concat1max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output_layer/Softmax:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer = Dense(1, activation='softmax', name='output_layer')(concat1max)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_layer:0\", shape=(?, 56), dtype=float32) at layer \"input_layer\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-940d28c5b884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mconcat1max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (? 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1812\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_layer:0\", shape=(?, 56), dtype=float32) at layer \"input_layer\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# write in one cell \n",
    "\n",
    "#=======================Build model=========================\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Input \n",
    "sequence_length = x_test.shape[1] # 56\n",
    "input_shape = (sequence_length,)\n",
    "input_layer = Input(shape=input_shape, name='input_layer') # (?, 56)\n",
    "\n",
    "# Embedding layer, (?, 56, 50)\n",
    "embedding_layer = Embedding(input_dim=len(vocabulary_inv), output_dim=embedding_dim,\n",
    "                      input_length=sequence_length, name='embedding_layer')(input_layer)\n",
    "\n",
    "# CNN, iterate filter_size\n",
    "conv_blocks = []\n",
    "for fz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                  kernel_size=fz, # 3 means 3 words\n",
    "                  padding='valid', # valid means no padding\n",
    "                  strides=1, # see explnation above\n",
    "                  activation='relu',\n",
    "                  use_bias=True)(drop_layer) \n",
    "    conv = GlobalMaxPooling1D()(conv) # 1-Max pooling \n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "\n",
    "concat1max = Concatenate()(conv_blocks) # (? 20)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
