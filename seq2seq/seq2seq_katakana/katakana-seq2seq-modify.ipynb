{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/eng-katakana.csv', header=None, names=['eng', 'katakana'])\n",
    "data = data.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>katakana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11206</th>\n",
       "      <td>Dorogobuzh</td>\n",
       "      <td>ドロゴブージ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80376</th>\n",
       "      <td>Gail Hopkins</td>\n",
       "      <td>ゲイル・ホプキンス</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38108</th>\n",
       "      <td>Novatek</td>\n",
       "      <td>ノヴァテク</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29960</th>\n",
       "      <td>Gyula Cseszneky</td>\n",
       "      <td>チェスネキー・ジュラ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>Occhieppo Superiore</td>\n",
       "      <td>オッキエッポ・スペリオーレ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng       katakana\n",
       "11206           Dorogobuzh         ドロゴブージ\n",
       "80376         Gail Hopkins      ゲイル・ホプキンス\n",
       "38108              Novatek          ノヴァテク\n",
       "29960      Gyula Cseszneky     チェスネキー・ジュラ\n",
       "22295  Occhieppo Superiore  オッキエッポ・スペリオーレ"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dorogobuzh', 'gail hopkins', 'novatek']\n",
      "['ドロゴブージ', 'ゲイル・ホプキンス', 'ノヴァテク']\n"
     ]
    }
   ],
   "source": [
    "data_input = [s.lower() for s in data['eng']]\n",
    "data_output = [s for s in data['katakana']]\n",
    "\n",
    "print(data_input[0:3])\n",
    "print(data_output[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75082\n",
      "32179\n"
     ]
    }
   ],
   "source": [
    "# Split train and test\n",
    "training_rate = 0.7\n",
    "train_len = int(len(data) * training_rate)\n",
    "training_input = data_input[:train_len]\n",
    "training_output = data_output[:train_len]\n",
    "validation_input = data_input[train_len:]\n",
    "validation_output = data_output[train_len:]\n",
    "\n",
    "print(len(training_input))\n",
    "print(len(validation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding character input\n",
    "\n",
    "We will create a character dictionary and encode the title from a string (a sequence of character) into a sequence of IDs. We will also create the reverse dictionary that will be used for getting the result later.\n",
    "\n",
    "Note that in practice, we must not build the dictionary from all data (`data_input` and `data_output`), but only use the training set (`training_input` and `training_output`). We also have to handle out-of-dictionary characters. However, for now, I will skip that part.\n",
    "\n",
    "Note:\n",
    "- We will use 0 for padding and 1 for 'START'. So, `count` starts from 2. \n",
    "- This is to take advantage of `mask_zero=True` feature for Embedding Layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English character dict size: 54\n",
      "Katakana character dict size: 89\n",
      "{'l': 2, 'a': 3, 'x': 4, 'u': 5, 'ŵ': 6, 'ê': 7, 'ź': 8, 'ý': 9, '9': 10, 'j': 11, 'w': 12, 'v': 13, 'ò': 14, 'ż': 15, 'õ': 16, 'ó': 17, 't': 18, 'm': 19, 'ú': 20, 'ù': 21, '0': 22, 'c': 23, 'ü': 24, 'q': 25, '5': 26, 's': 27, 'þ': 28, 'o': 29, '3': 30, 'r': 31, '1': 32, ' ': 33, '2': 34, '8': 35, 'ž': 36, 'd': 37, 'k': 38, 'y': 39, '4': 40, 'p': 41, 'e': 42, '6': 43, 'n': 44, '7': 45, 'f': 46, 'b': 47, 'i': 48, 'z': 49, 'ļ': 50, 'ľ': 51, 'g': 52, 'h': 53}\n",
      "{1: 'START', 2: 'l', 3: 'a', 4: 'x', 5: 'u', 6: 'ŵ', 7: 'ê', 8: 'ź', 9: 'ý', 10: '9', 11: 'j', 12: 'w', 13: 'v', 14: 'ò', 15: 'ż', 16: 'õ', 17: 'ó', 18: 't', 19: 'm', 20: 'ú', 21: 'ù', 22: '0', 23: 'c', 24: 'ü', 25: 'q', 26: '5', 27: 's', 28: 'þ', 29: 'o', 30: '3', 31: 'r', 32: '1', 33: ' ', 34: '2', 35: '8', 36: 'ž', 37: 'd', 38: 'k', 39: 'y', 40: '4', 41: 'p', 42: 'e', 43: '6', 44: 'n', 45: '7', 46: 'f', 47: 'b', 48: 'i', 49: 'z', 50: 'ļ', 51: 'ľ', 52: 'g', 53: 'h'}\n"
     ]
    }
   ],
   "source": [
    "START_CHAR_CODE = 1\n",
    "\n",
    "def encode_characters(titles):\n",
    "    count = 2\n",
    "    encoding = {}\n",
    "    decoding = {1: 'START'}\n",
    "    for c in set([c for title in titles for c in title]):\n",
    "        encoding[c] = count\n",
    "        decoding[count] = c\n",
    "        count += 1\n",
    "    return encoding, decoding, count\n",
    "\n",
    "\n",
    "input_encoding, input_decoding, input_dict_size = encode_characters(data_input)\n",
    "output_encoding, output_decoding, output_dict_size = encode_characters(data_output)\n",
    "\n",
    "\n",
    "print('English character dict size:', input_dict_size)\n",
    "print('Katakana character dict size:', output_dict_size)\n",
    "\n",
    "print(input_encoding)\n",
    "print(input_decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [[37. 29. 31. ...  0.  0.  0.]\n",
      " [52.  3. 48. ...  0.  0.  0.]\n",
      " [44. 29. 13. ...  0.  0.  0.]\n",
      " ...\n",
      " [11. 29. 53. ...  0.  0.  0.]\n",
      " [52. 44.  3. ...  0.  0.  0.]\n",
      " [42. 53. 31. ...  0.  0.  0.]]\n",
      "output [[85. 61. 41. ...  0.  0.  0.]\n",
      " [21. 30. 39. ...  0.  0.  0.]\n",
      " [46. 12.  9. ...  0.  0.  0.]\n",
      " ...\n",
      " [55.  3. 35. ...  0.  0.  0.]\n",
      " [58.  5.  6. ...  0.  0.  0.]\n",
      " [33. 88. 54. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def transform(encoding, data, vector_size):\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            transformed_data[i][j] = encoding[data[i][j]]\n",
    "    return transformed_data\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "encoded_training_input = transform(input_encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(output_encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "encoded_validation_input = transform(input_encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(output_encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('input', encoded_training_input)\n",
    "print('output', encoded_training_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "First, we will use [Embedding layer](https://keras.io/layers/embeddings/) to transform input char-id sequence into dense vectors.  \n",
    "\n",
    "The input vectors will be passed to a [Recurrent layer](https://keras.io/layers/recurrent/) (we use LSTM) that will transform the vectors of each input character to a single output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(?, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 64)\n",
      "(?, 64)\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder = Embedding(input_dict_size, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "print(encoder.get_shape())\n",
    "encoder = LSTM(64)(encoder)\n",
    "print(encoder.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Our decoder generate Katakana sequence (as a softmax prediction) on characrter at the time. Every generated output at decoding step will be passed back as an input of the decoder to generate the next output.\n",
    "\n",
    "Similar to the encoder, the input will be passed to an Embedding layer to transform the input into dense vectors and pass them to LSTM.\n",
    "\n",
    "We will use the encoder's output to initialize decoder state (`initial_state`).\n",
    "\n",
    "The final layer will be (time distributed) Dense layer that will produce the softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 64)\n",
      "(?, ?, 64)\n",
      "(?, 20, 89)\n"
     ]
    }
   ],
   "source": [
    "decoder = Embedding(output_dict_size, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "print(decoder.get_shape())\n",
    "decoder = LSTM(64, return_sequences=True)(decoder, initial_state=[encoder, encoder])\n",
    "print(decoder.get_shape())\n",
    "decoder = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)\n",
    "\n",
    "print(decoder.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75082, 20)\n",
      "(75082, 20)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_training_input.shape)\n",
    "print(encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input [[37. 29. 31. 29. 52. 29. 47.  5. 49. 53.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "decoder input [[ 1. 85. 61. 41. 75. 19. 55.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n",
      "decoder output [[85 61 41 75 19 55  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "decoder output (one-hot) [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Encoder Input\n",
    "training_encoder_input = encoded_training_input\n",
    "\n",
    "# Decoder Input (need padding py START_CHAR_CODE)\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1] # offset one timestpe\n",
    "training_decoder_input[:, 0] = START_CHAR_CODE # first timestep is 1, means START\n",
    "\n",
    "# Decoder Output (one-hot encode)\n",
    "training_decoder_output = np.eye(output_dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "print('encoder input', training_encoder_input[:1])\n",
    "print('decoder input', training_decoder_input[:1])\n",
    "print('decoder output', training_decoder_output[:1].argmax(axis=2))\n",
    "print('decoder output (one-hot)', training_decoder_output[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75082, 20)\n",
      "(75082, 20)\n",
      "(75082, 20, 89)\n"
     ]
    }
   ],
   "source": [
    "print(training_encoder_input.shape)\n",
    "print(training_decoder_input.shape)\n",
    "print(training_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = START_CHAR_CODE\n",
    "validation_decoder_output = np.eye(output_dict_size)[encoded_validation_output.astype('int')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isfile('s2s_modify.h5'):\n",
    "#     model = load_model('s2s_modify.h5')\n",
    "# else:\n",
    "#     model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "#           validation_split=0.11,\n",
    "#           verbose=1,\n",
    "#           batch_size=64,\n",
    "#           epochs=2)\n",
    "    \n",
    "# model.save('s2s_modify.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66822 samples, validate on 8260 samples\n",
      "Epoch 1/20\n",
      "66822/66822 [==============================] - 55s 828us/step - loss: 0.0368 - val_loss: 0.0354\n",
      "Epoch 2/20\n",
      "66822/66822 [==============================] - 49s 737us/step - loss: 0.0339 - val_loss: 0.0328\n",
      "Epoch 3/20\n",
      "66822/66822 [==============================] - 59s 889us/step - loss: 0.0314 - val_loss: 0.0305\n",
      "Epoch 4/20\n",
      "66822/66822 [==============================] - 52s 776us/step - loss: 0.0293 - val_loss: 0.0286\n",
      "Epoch 5/20\n",
      "66822/66822 [==============================] - 52s 782us/step - loss: 0.0277 - val_loss: 0.0272\n",
      "Epoch 6/20\n",
      "66822/66822 [==============================] - 50s 742us/step - loss: 0.0264 - val_loss: 0.0261\n",
      "Epoch 7/20\n",
      "66822/66822 [==============================] - 49s 736us/step - loss: 0.0252 - val_loss: 0.0249\n",
      "Epoch 8/20\n",
      "66822/66822 [==============================] - 51s 758us/step - loss: 0.0243 - val_loss: 0.0243\n",
      "Epoch 9/20\n",
      "66822/66822 [==============================] - 63s 944us/step - loss: 0.0235 - val_loss: 0.0236\n",
      "Epoch 10/20\n",
      "66822/66822 [==============================] - 57s 851us/step - loss: 0.0228 - val_loss: 0.0229\n",
      "Epoch 11/20\n",
      "66822/66822 [==============================] - 66s 985us/step - loss: 0.0222 - val_loss: 0.0223\n",
      "Epoch 12/20\n",
      "66822/66822 [==============================] - 66s 995us/step - loss: 0.0217 - val_loss: 0.0220\n",
      "Epoch 13/20\n",
      "66822/66822 [==============================] - 59s 885us/step - loss: 0.0212 - val_loss: 0.0216\n",
      "Epoch 14/20\n",
      "66822/66822 [==============================] - 60s 896us/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 15/20\n",
      "66822/66822 [==============================] - 68s 1ms/step - loss: 0.0204 - val_loss: 0.0208\n",
      "Epoch 16/20\n",
      "66822/66822 [==============================] - 68s 1ms/step - loss: 0.0201 - val_loss: 0.0207\n",
      "Epoch 17/20\n",
      "66822/66822 [==============================] - 65s 974us/step - loss: 0.0198 - val_loss: 0.0204\n",
      "Epoch 18/20\n",
      "66822/66822 [==============================] - 51s 766us/step - loss: 0.0195 - val_loss: 0.0200\n",
      "Epoch 19/20\n",
      "66822/66822 [==============================] - 51s 768us/step - loss: 0.0193 - val_loss: 0.0199\n",
      "Epoch 20/20\n",
      "66822/66822 [==============================] - 51s 762us/step - loss: 0.0191 - val_loss: 0.0197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117591828>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "      validation_split=0.11,\n",
    "      verbose=1,\n",
    "      batch_size=64,\n",
    "      epochs=20,\n",
    "      callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "plot_model(model, show_shapes=True, to_file='s2s_modify.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "During the testing or after deploy the model, to generate the output we will use \"greedy\" generating approach, which is generating one output at a time by maximize softmax score and feed the output back as the next decoder input character. \n",
    "\n",
    "We won't use [beam-search decoding](https://www.quora.com/Why-is-beam-search-required-in-sequence-to-sequence-transduction-using-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text):\n",
    "    encoder_input = transform(input_encoding, [text.lower()], 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = START_CHAR_CODE\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return decoder_input[:,1:]\n",
    "\n",
    "def decode(decoding, sequence):\n",
    "    text = ''\n",
    "    for i in sequence:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += output_decoding[i]\n",
    "    return text\n",
    "\n",
    "def to_katakana(text):\n",
    "    decoder_output = generate(text)\n",
    "    return decode(output_decoding, decoder_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is trained correctly, typical names should be translate correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James ジェームス\n",
      "John ジョン\n",
      "Robert ロバート\n",
      "Mary マリー\n",
      "Patricia パトリシア\n",
      "Linda リンダ\n"
     ]
    }
   ],
   "source": [
    "common_american_names = ['James', 'John', 'Robert', 'Mary', 'Patricia', 'Linda']\n",
    "for name in common_american_names:\n",
    "    print(name, to_katakana(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we train the model with mostly people and places names, some English words may not be written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コンプター\n",
      "タッキー\n"
     ]
    }
   ],
   "source": [
    "print(to_katakana('computer'))\n",
    "print(to_katakana('taxi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
