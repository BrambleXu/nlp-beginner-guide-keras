{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword level representation\n",
    "\n",
    "\n",
    "In this notebook, we will preprocess the data to represent sentences in a subword level. The data set is `ag_news`, same with `char-level-cnn` [project](https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/char-level-cnn). The reason that I create [nlp-beginner-guide-keras](https://github.com/BrambleXu/nlp-beginner-guide-keras) is to learn different techniques, so here we use a different approach to do the preprocess. We will use subword level word representation, instead of character level word representation.\n",
    "\n",
    "\n",
    "## What is subword level representation\n",
    "\n",
    "\n",
    "\n",
    "## Why use subword level representation\n",
    "\n",
    "\n",
    "## How to preprocess \n",
    "\n",
    "As for the preprocessing, you can find detail explanation in this notebook [subword-preprocess](https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/char-level-rnn/notebooks/subword-preprocess.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from data_helpers import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#=======================All Preprocessing====================\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data_source = '../../char-level-cnn/data/ag_news_csv/train.csv'\n",
    "test_data_source = '../../char-level-cnn/data/ag_news_csv/test.csv'\n",
    "train_df = pd.read_csv(train_data_source, header=None)\n",
    "test_df = pd.read_csv(test_data_source, header=None)\n",
    "\n",
    "# concatenate column 1 and column 2 as one text\n",
    "for df in [train_df, test_df]:\n",
    "    df[1] = df[1] + df[2]\n",
    "    df = df.drop([2], axis=1)\n",
    "    \n",
    "# convert string to lower case \n",
    "train_texts = train_df[1].values \n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "test_texts = test_df[1].values \n",
    "test_texts = [s.lower() for s in test_texts]\n",
    "\n",
    "# replace all digits with 0\n",
    "import re\n",
    "train_texts = [re.sub('\\d', '0', s) for s in train_texts]\n",
    "test_texts = [re.sub('\\d', '0', s) for s in test_texts]\n",
    "\n",
    "# replace all URLs with <url> \n",
    "url_reg  = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "train_texts = [re.sub(url_reg, '<url>', s) for s in train_texts]\n",
    "test_texts = [re.sub(url_reg, '<url>', s) for s in test_texts]\n",
    "\n",
    "# Convert string to subword, this process may take several minutes\n",
    "bpe = BPE(\"../pre-trained-model/en.wiki.bpe.op25000.vocab\")\n",
    "train_texts = [bpe.encode(s) for s in train_texts]\n",
    "test_texts = [bpe.encode(s) for s in test_texts]\n",
    "\n",
    "# Build vocab, {token: index}\n",
    "vocab = {}\n",
    "for i, token in enumerate(bpe.words):\n",
    "    vocab[token] = i + 1\n",
    "    \n",
    "# Convert subword to index, function version \n",
    "def subword2index(texts, vocab):\n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "        s = s.split()\n",
    "        one_line = []\n",
    "        for word in s:\n",
    "            if word not in vocab.keys():\n",
    "                one_line.append(vocab['unk'])\n",
    "            else:\n",
    "                one_line.append(vocab[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences\n",
    "\n",
    "# Convert train and test \n",
    "train_sentences = subword2index(train_texts, vocab)\n",
    "test_sentences = subword2index(test_texts, vocab)\n",
    "\n",
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_data = pad_sequences(train_sentences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_sentences, maxlen=1014, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#=======================Get classes================\n",
    "train_classes = train_df[0].values\n",
    "train_class_list = [x-1 for x in train_classes]\n",
    "test_classes = test_df[0].values\n",
    "test_class_list = [x-1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(train_class_list)\n",
    "test_classes = to_categorical(test_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join, exists, split\n",
    "\n",
    "# data_dir = '../preprocessed-data/dataset'\n",
    "# train_x = 'train_data.npy'\n",
    "# train_y = 'train_class.npy'\n",
    "# test_x = 'test_data.npy'\n",
    "# test_y = 'test_classes.npy'\n",
    "\n",
    "# # np.save(join(data_dir, train_x), train_data) \n",
    "# np.savez(data_dir, x_train=train_data, y_train=train_classes, x_test=test_data, y_test=test_classes)\n",
    "# # This file is too big, 519.6MB\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# x = pd.HDFStore(\"dataset.hdf\")\n",
    "# x.append(\"train_data\", pd.DataFrame(train_data)) # <-- This will take a while.\n",
    "# x.append(\"test_data\", pd.DataFrame(test_data)) # <-- This will take a while.\n",
    "# x.close()\n",
    "# # This will also output a datafile bigger than 500MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5323,    68, 24904, ...,     0,     0,     0],\n",
       "       [ 3226,    84,    51, ...,     0,     0,     0],\n",
       "       [18658,    36,  6182, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [21745,    18,   313, ...,     0,     0,     0],\n",
       "       [15235, 24915, 24889, ...,     0,     0,     0],\n",
       "       [  591,   302,  2622, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding layer wegihts\n",
    "\n",
    "In order to use the embedding weights we first to load the subword embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"../pre-trained-model/en.wiki.bpe.op25000.d50.w2v.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "▁t\n",
      "▁a\n",
      "he\n"
     ]
    }
   ],
   "source": [
    "for i, subword in enumerate(vocab):\n",
    "    print(subword)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.399702, -0.769862, -0.06641 , -0.211852, -0.359098, -0.055825,\n",
       "        0.4286  , -0.256576, -0.086343,  0.406772, -0.072157, -0.174386,\n",
       "        0.398903, -0.040825, -0.155359,  0.048774, -0.238695,  0.024354,\n",
       "       -0.347787,  0.081793,  0.141403,  0.08835 , -0.070075,  0.110401,\n",
       "        0.003846, -0.265394,  0.724276, -0.523481, -0.162674,  0.147213,\n",
       "       -0.209789, -0.132434, -0.067623,  0.691781,  0.421201, -0.047779,\n",
       "        0.397612, -0.279393, -0.967681,  0.55612 , -0.042962, -0.3673  ,\n",
       "        0.314757,  0.114486, -0.278512, -0.042936, -0.020144,  0.100965,\n",
       "        0.181277,  0.040286], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.047345, -0.813617, -0.402143,  0.163767,  3.029769,  0.466452,\n",
       "       -0.859536,  0.912698,  0.513252, -0.082041,  1.04137 ,  1.15992 ,\n",
       "        0.183564,  0.32676 , -0.983799, -0.744597,  0.547359,  0.341305,\n",
       "        0.239759,  0.953342, -0.474623, -1.014153,  0.780751, -0.970756,\n",
       "       -0.436472,  0.998653, -1.763717,  0.156439, -0.411622,  0.544716,\n",
       "       -0.902719, -0.825915,  0.549098, -0.080528, -1.215276, -0.113391,\n",
       "       -0.735994, -0.501781,  1.573995, -0.817193,  0.087332,  0.090806,\n",
       "        0.293357, -0.444164,  0.192026, -0.580188,  0.51405 , -0.857277,\n",
       "        1.569506,  0.143075], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n",
      "▁distric\n",
      "ptember\n",
      "bruary\n",
      "▁performan\n",
      "orporated\n",
      "▁headqu\n",
      "▁attem\n",
      "▁mathem\n",
      "▁passeng\n",
      "uguese\n",
      "▁azerbai\n",
      "▁compris\n",
      "urday\n",
      "▁emplo\n",
      "▁portra\n",
      "▁thous\n",
      "▁lithu\n",
      "▁leban\n",
      "▁councill\n",
      "▁specim\n",
      "▁molec\n",
      "▁entrepren\n",
      "▁predecess\n",
      "▁glouc\n",
      "▁earthqu\n",
      "▁istan\n",
      "imination\n",
      "▁infloresc\n",
      "▁ingred\n",
      "chiidae\n",
      "▁sofl\n",
      "ürttemberg\n",
      "▁practition\n",
      "echua\n",
      "eteries\n",
      "bridgeshire\n",
      "▁nudi\n",
      "rzys\n",
      "tokrzys\n",
      "uchestan\n",
      "▁taekw\n",
      "kopol\n",
      "giluyeh\n",
      "▁fute\n",
      "ivisie\n",
      "marthen\n",
      "▁gillesp\n",
      "aziland\n",
      "scray\n",
      "alandhar\n",
      "azulu\n",
      "alisco\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # (25001, 50)\n",
    "\n",
    "for subword, i in vocab.items():\n",
    "    if subword in model.vocab:\n",
    "        embedding_vector = model[subword]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_weights[i] = embedding_vector\n",
    "    else:\n",
    "        print(subword) # print the subword in vocab but not in model\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(25001, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# parameter \n",
    "input_size = 1014\n",
    "embedding_size = 50\n",
    "\n",
    "num_of_classes = 4\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "embedding_layer = Embedding(len(vocab)+1,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=input_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Flatten\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1014)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1014, 50)          1250050   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 1014, 256)         314368    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 1014, 256)         525312    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 259584)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              265815040 \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 268,958,470\n",
      "Trainable params: 268,958,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(input_size,))\n",
    "embedded_sequence = embedding_layer(inputs)\n",
    "x = LSTM(256, return_sequences=True, activation='relu')(embedded_sequence)\n",
    "x = LSTM(256, return_sequences=True, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(dropout_p)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(dropout_p)(x)\n",
    "prediction = Dense(num_of_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1075s 1s/step - loss: 0.8297 - acc: 0.6600 - val_loss: 0.7974 - val_acc: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aaaf76b00>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the data \n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices][:1000]\n",
    "y_train = train_classes[indices][:1000]\n",
    "\n",
    "x_test = test_data[:100]\n",
    "y_test = test_classes[:100]\n",
    "\n",
    "# training\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
