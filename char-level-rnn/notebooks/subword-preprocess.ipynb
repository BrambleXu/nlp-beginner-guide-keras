{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword level representation\n",
    "\n",
    "\n",
    "In this notebook, we will preprocess the data to represent sentences in a subword level. \n",
    "\n",
    "## byte-pair-encoding (BPE) \n",
    "\n",
    "\n",
    "After doing some research about subword level word representation, here I found some useful tools.\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece) \n",
    "- [BPEmb](https://github.com/bheinzerling/bpemb)\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer tool, and it supports multiple subword algorithms, like byte-pair-encoding (BPE) and unigram language model. BPEmb is a collection of pre-trained subword embeddings in 275 languages, and it support SentencePiece. But I had some trouble to install the SentencePiece.\n",
    "\n",
    "Because I am using OSX, I have to build SentencePiece from source with C++. After run the command: `brew install protobuf autoconf automake libtool`, it returns the error shows `Error: Failed to download resource \"pkg-config\"`. After try some ways I still can not install SentencePiece successfully. Fortunately in the BPEmb page, there are is a script that could get the subword vectors without using SentencePiece. Here we use this script to get the subword vector.\n",
    "\n",
    "[On-the-fly conversion to subwords in Python](https://github.com/bheinzerling/bpemb/issues/10). This version is easier to use: [bpe.py](https://github.com/bheinzerling/bpemb/blob/master/bpe.py)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "class BPE(object):\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        with open(vocab_file, encoding=\"utf8\") as f:\n",
    "            self.words = [l.split()[0] for l in f]\n",
    "            log_len = log(len(self.words))\n",
    "            self.wordcost = {\n",
    "                k: log((i+1) * log_len)\n",
    "                for i, k in enumerate(self.words)}\n",
    "            self.maxword = max(len(x) for x in self.words)\n",
    "\n",
    "    def encode(self, s):\n",
    "        \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "        without spaces.\"\"\"\n",
    "\n",
    "        s = s.replace(\" \", \"▁\")\n",
    "\n",
    "        # Find the best match for the i first characters, assuming cost has\n",
    "        # been built for the i-1 first characters.\n",
    "        # Returns a pair (match_cost, match_length).\n",
    "        def best_match(i):\n",
    "            candidates = enumerate(reversed(cost[max(0, i - self.maxword):i]))\n",
    "            return min(\n",
    "                (c + self.wordcost.get(s[i-k-1:i], 9e999), k+1)\n",
    "                for k, c in candidates)\n",
    "\n",
    "        # Build the cost array.\n",
    "        cost = [0]\n",
    "        for i in range(1, len(s) + 1):\n",
    "            c, k = best_match(i)\n",
    "            cost.append(c)\n",
    "\n",
    "        # Backtrack to recover the minimal-cost string.\n",
    "        out = []\n",
    "        i = len(s)\n",
    "        while i > 0:\n",
    "            c, k = best_match(i)\n",
    "            assert c == cost[i]\n",
    "            out.append(s[i-k:i])\n",
    "\n",
    "            i -= k\n",
    "\n",
    "        return \" \".join(reversed(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three attributes in the BPE class:\n",
    "- `BPE.words`: a list of all subword token, we can take this as a dictionary\n",
    "- `BPE.wordcost`: a dict contain `log(len(elf.words))` for each word\n",
    "- `BPE.maxword`: the most longest length of token in `BPE.words`\n",
    "\n",
    "We take an example to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁this ▁is ▁our ▁house ▁in ▁boom ch ak al aka\n"
     ]
    }
   ],
   "source": [
    "bpe = BPE(\"../pre_trained_model/en.wiki.bpe.op25000.vocab\")\n",
    "print(bpe.encode(' this is our house in boomchakalaka'))  \n",
    "# >>> ▁this ▁is ▁our ▁house ▁in ▁boom ch ak al aka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<s>', '</s>', '▁t', '▁a', 'he', 'in', '▁the', '00', 'er']\n",
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "▁t\n",
      "▁a\n",
      "he\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# see the attributes in the BPE class\n",
    "print(bpe.words[:10])\n",
    "\n",
    "for i, token in enumerate(bpe.wordcost):\n",
    "    print(token)\n",
    "    if i > 4:\n",
    "        break\n",
    "        \n",
    "print(bpe.maxword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we precess our sentences to get the subword level representation.\n",
    "\n",
    "## Preprocess \n",
    "\n",
    "- Load data \n",
    "- Convert string to subword\n",
    "- Convert subword to index \n",
    "- Padding \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================Load data=========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data_source = '../../char-level-cnn/data/ag_news_csv/train.csv'\n",
    "test_data_source = '../../char-level-cnn/data/ag_news_csv/test.csv'\n",
    "train_df = pd.read_csv(train_data_source, header=None)\n",
    "test_df = pd.read_csv(test_data_source, header=None)\n",
    "# concatenate column 1 and column 2 as one text\n",
    "for df in [train_df, test_df]:\n",
    "    df[1] = df[1] + df[2]\n",
    "    df = df.drop([2], axis=1)\n",
    "    \n",
    "# convert string to lower case \n",
    "train_texts = train_df[1].values \n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "test_texts = test_df[1].values \n",
    "test_texts = [s.lower() for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall st. bears claw back into the black (reuters)reuters - short-sellers, wall street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "fears for t n pension after talksunions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(test_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all digits with 0\n",
    "import re\n",
    "train_texts = [re.sub('\\d', '0', s) for s in train_texts]\n",
    "test_texts = [re.sub('\\d', '0', s) for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall st. bears claw back into the black (reuters)reuters - short-sellers, wall street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "fears for t n pension after talksunions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(test_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all URLs with <url> \n",
    "url_reg  = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "train_texts = [re.sub(url_reg, '<url>', s) for s in train_texts]\n",
    "test_texts = [re.sub(url_reg, '<url>', s) for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall st. bears claw back into the black (reuters)reuters - short-sellers, wall street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "fears for t n pension after talksunions representing workers at turner   newall say they are 'disappointed' after talks with stricken parent firm federal mogul.\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(test_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for `re.MULTILINE`, you can see the explanation [here](https://teamtreehouse.com/community/dont-quite-understand-the-use-of-remultiline) and [hear](http://messefor.hatenablog.com/entry/2017/01/15/215722)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to subword, this process may take several minutes\n",
    "train_texts = [bpe.encode(s) for s in train_texts]\n",
    "test_texts = [bpe.encode(s) for s in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall ▁st . ▁bears ▁c law ▁back ▁into ▁the ▁black ▁( re uters ) re uters ▁- ▁short - sel lers , ▁wall ▁street ' s ▁d wind ling \\ b a n d ▁ o f ▁ u l t r a - c y n i c s , ▁ a r e ▁ s e e i n g ▁ g r e e n ▁ a g a i n .\n",
      "fe ars ▁for ▁t ▁n ▁pension ▁after ▁talks un ions ▁representing ▁workers ▁at ▁turner ▁ ▁ ▁new all ▁say ▁they ▁are ▁' dis app ointed ' ▁after ▁talks ▁with ▁strick en ▁parent ▁firm ▁federal ▁mog ul .\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(test_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab, {token: index}\n",
    "vocab = {}\n",
    "for i, token in enumerate(bpe.words):\n",
    "    vocab[token] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> 1\n",
      "<s> 2\n",
      "</s> 3\n",
      "▁t 4\n",
      "▁a 5\n",
      "he 6\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(vocab.items()):\n",
    "    print(key, value)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert subword to index \n",
    "# train_sentences = []\n",
    "# for s in train_texts:\n",
    "#     s = s.split()\n",
    "#     one_line = []\n",
    "#     for word in s:\n",
    "#         if word not in vocab.keys():\n",
    "#             one_line.append(vocab['<unk>'])\n",
    "#         else:\n",
    "#             one_line.append(vocab[word])\n",
    "#     train_sentences.append(one_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5323, 68, 24904, 6039, 13, 5012, 1025, 549, 8, 1237, 72, 14, 21182, 24912, 14, 21182, 865, 1144, 24910, 3065, 5583, 24905, 2227, 1230, 24915, 24889, 37, 9683, 1307, 1, 24901, 24884, 24887, 24893, 24882, 24888, 24898, 24882, 24897, 24892, 24885, 24890, 24884, 24910, 24894, 24903, 24887, 24886, 24894, 24889, 24905, 24882, 24884, 24890, 24883, 24882, 24889, 24883, 24883, 24886, 24887, 24900, 24882, 24900, 24890, 24883, 24883, 24887, 24882, 24884, 24900, 24884, 24886, 24887, 24904]\n"
     ]
    }
   ],
   "source": [
    "# print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert subword to index, function version \n",
    "def subword2index(texts, vocab):\n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "        s = s.split()\n",
    "        one_line = []\n",
    "        for word in s:\n",
    "            if word not in vocab.keys():\n",
    "                one_line.append(vocab['unk'])\n",
    "            else:\n",
    "                one_line.append(vocab[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences\n",
    "    \n",
    "# Convert train and test \n",
    "train_sentences = subword2index(train_texts, vocab)\n",
    "test_sentences = subword2index(test_texts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "[5323, 68, 24904, 6039, 13, 5012, 1025, 549, 8, 1237, 72, 14, 21182, 24912, 14, 21182, 865, 1144, 24910, 3065, 5583, 24905, 2227, 1230, 24915, 24889, 37, 9683, 1307, 2837, 24901, 24884, 24887, 24893, 24882, 24888, 24898, 24882, 24897, 24892, 24885, 24890, 24884, 24910, 24894, 24903, 24887, 24886, 24894, 24889, 24905, 24882, 24884, 24890, 24883, 24882, 24889, 24883, 24883, 24886, 24887, 24900, 24882, 24900, 24890, 24883, 24883, 24887, 24882, 24884, 24900, 24884, 24886, 24887, 24904]\n",
      "7600\n",
      "[3225, 1572, 71, 4, 48, 15602, 296, 13706, 47, 225, 3499, 4629, 95, 7898, 24882, 24882, 232, 130, 3651, 422, 157, 1186, 8429, 1383, 23271, 24915, 296, 13706, 105, 20439, 25, 7619, 3430, 2352, 13573, 97, 24904]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))\n",
    "print(train_sentences[0])\n",
    "print(len(test_sentences))\n",
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sentences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_sentences, maxlen=1014, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "[ 5323    68 24904 ...     0     0     0]\n",
      "1014\n",
      "[3225 1572   71 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "print(train_data[0])\n",
    "print(len(train_data[0]))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================Get classes================\n",
    "train_classes = train_df[0].values\n",
    "train_class_list = [x-1 for x in train_classes]\n",
    "test_classes = test_df[0].values\n",
    "test_class_list = [x-1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(train_class_list)\n",
    "test_classes = to_categorical(test_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_classes)\n",
    "print(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write all code together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "from math import log\n",
    "\n",
    "class BPE(object):\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        with open(vocab_file, encoding=\"utf8\") as f:\n",
    "            self.words = [l.split()[0] for l in f]\n",
    "            log_len = log(len(self.words))\n",
    "            self.wordcost = {\n",
    "                k: log((i+1) * log_len)\n",
    "                for i, k in enumerate(self.words)}\n",
    "            self.maxword = max(len(x) for x in self.words)\n",
    "\n",
    "    def encode(self, s):\n",
    "        \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "        without spaces.\"\"\"\n",
    "\n",
    "        s = s.replace(\" \", \"▁\")\n",
    "\n",
    "        # Find the best match for the i first characters, assuming cost has\n",
    "        # been built for the i-1 first characters.\n",
    "        # Returns a pair (match_cost, match_length).\n",
    "        def best_match(i):\n",
    "            candidates = enumerate(reversed(cost[max(0, i - self.maxword):i]))\n",
    "            return min(\n",
    "                (c + self.wordcost.get(s[i-k-1:i], 9e999), k+1)\n",
    "                for k, c in candidates)\n",
    "\n",
    "        # Build the cost array.\n",
    "        cost = [0]\n",
    "        for i in range(1, len(s) + 1):\n",
    "            c, k = best_match(i)\n",
    "            cost.append(c)\n",
    "\n",
    "        # Backtrack to recover the minimal-cost string.\n",
    "        out = []\n",
    "        i = len(s)\n",
    "        while i > 0:\n",
    "            c, k = best_match(i)\n",
    "            assert c == cost[i]\n",
    "            out.append(s[i-k:i])\n",
    "\n",
    "            i -= k\n",
    "\n",
    "        return \" \".join(reversed(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================All Preprocessing====================\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data_source = '../../char-level-cnn/data/ag_news_csv/train.csv'\n",
    "test_data_source = '../../char-level-cnn/data/ag_news_csv/test.csv'\n",
    "train_df = pd.read_csv(train_data_source, header=None)\n",
    "test_df = pd.read_csv(test_data_source, header=None)\n",
    "# concatenate column 1 and column 2 as one text\n",
    "for df in [train_df, test_df]:\n",
    "    df[1] = df[1] + df[2]\n",
    "    df = df.drop([2], axis=1)\n",
    "    \n",
    "# convert string to lower case \n",
    "train_texts = train_df[1].values \n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "test_texts = test_df[1].values \n",
    "test_texts = [s.lower() for s in test_texts]\n",
    "\n",
    "# replace all digits with 0\n",
    "import re\n",
    "train_texts = [re.sub('\\d', '0', s) for s in train_texts]\n",
    "test_texts = [re.sub('\\d', '0', s) for s in test_texts]\n",
    "\n",
    "# replace all URLs with <url> \n",
    "url_reg  = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "train_texts = [re.sub(url_reg, '<url>', s) for s in train_texts]\n",
    "test_texts = [re.sub(url_reg, '<url>', s) for s in test_texts]\n",
    "\n",
    "# Convert string to subword, this process may take several minutes\n",
    "train_texts = [bpe.encode(s) for s in train_texts]\n",
    "test_texts = [bpe.encode(s) for s in test_texts]\n",
    "\n",
    "# Build vocab, {token: index}\n",
    "vocab = {}\n",
    "for i, token in enumerate(bpe.words):\n",
    "    vocab[token] = i + 1\n",
    "    \n",
    "# Convert subword to index, function version \n",
    "def subword2index(texts, vocab):\n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "        s = s.split()\n",
    "        one_line = []\n",
    "        for word in s:\n",
    "            if word not in vocab.keys():\n",
    "                one_line.append(vocab['unk'])\n",
    "            else:\n",
    "                one_line.append(vocab[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences\n",
    "\n",
    "# Convert train and test \n",
    "train_sentences = subword2index(train_texts, vocab)\n",
    "test_sentences = subword2index(test_texts, vocab)\n",
    "\n",
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_data = pad_sequences(train_sentences, maxlen=1014, padding='post')\n",
    "test_data = pad_sequences(test_sentences, maxlen=1014, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#=======================Get classes================\n",
    "train_classes = train_df[0].values\n",
    "train_class_list = [x-1 for x in train_classes]\n",
    "test_classes = test_df[0].values\n",
    "test_class_list = [x-1 for x in test_classes]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(train_class_list)\n",
    "test_classes = to_categorical(test_class_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
