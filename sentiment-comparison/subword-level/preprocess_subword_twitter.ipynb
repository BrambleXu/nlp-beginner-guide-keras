{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_helpers\n",
    "import pickle\n",
    "from data_helpers import TrainValTensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import CSVLogger\n",
    "from data_helpers import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0    - awww, that's a bummer.  you shoulda got da...       0\n",
      "1  is upset that he can't update his facebook by ...       0\n",
      "2   i dived many times for the ball. managed to s...       0\n",
      "3    my whole body feels itchy and like its on fire        0\n",
      "4   no, it's not behaving at all. i'm mad. why am...       0\n",
      "0    799997\n",
      "1    799995\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#==================Preprocess===================\n",
    "\n",
    "# Load data\n",
    "csv = '../data/twitter/clean_tweet_char.csv'\n",
    "df = pd.read_csv(csv, index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "# Delete Null row\n",
    "df = df.dropna()\n",
    "print(df.target.value_counts())\n",
    "\n",
    "\n",
    "x_text = df['text'].values\n",
    "y = df['target'].values\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert subword to index, function version\n",
    "def subword2index(texts, vocab):\n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "        s = s.split()\n",
    "        one_line = []\n",
    "        for word in s:\n",
    "            if word not in vocab.keys():\n",
    "                one_line.append(vocab['unk'])\n",
    "            else:\n",
    "                one_line.append(vocab[word])\n",
    "        sentences.append(one_line)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# replace all digits with 0\n",
    "import re\n",
    "\n",
    "train_texts = [re.sub('\\d', '0', s) for s in x_text]\n",
    "\n",
    "# replace all URLs with <url>\n",
    "url_reg = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "train_texts = [re.sub(url_reg, '<url>', s) for s in train_texts]\n",
    "\n",
    "# Convert string to subword, this process may take several minutes\n",
    "bpe = BPE(\"./pre-trained-model/en.wiki.bpe.op25000.vocab\")\n",
    "train_texts = [bpe.encode(s) for s in train_texts]\n",
    "\n",
    "# Build vocab, {token: index}\n",
    "vocab = {}\n",
    "for i, token in enumerate(bpe.words):\n",
    "    vocab[token] = i + 1\n",
    "\n",
    "# Convert train and test\n",
    "train_sentences = subword2index(train_texts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  364\n",
      "The min length is:  1\n",
      "The average length is:  20.11691870959355\n"
     ]
    }
   ],
   "source": [
    "# See subword level length\n",
    "length = [len(sent) for sent in train_sentences]\n",
    "print('The max length is: ', max(length))\n",
    "print('The min length is: ', min(length))\n",
    "print('The average length is: ', sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_data = pad_sequences(train_sentences, maxlen=364, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size is:  (1439992, 364)\n",
      "Validation data size is:  (160000, 364)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = train_data[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "print('Training data size is: ', x_train.shape)\n",
    "print('Validation data size is: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_tweet.csv\r\n",
      "clean_tweet_char.csv\r\n",
      "\u001b[31mtraining.1600000.processed.noemoticon.csv\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/twitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/twitter/preprocessed_dataset.npz'\n",
    "\n",
    "np.savez(data_dir, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "▁t\n",
      "▁a\n"
     ]
    }
   ],
   "source": [
    "for i, items in enumerate(vocab):\n",
    "    print(items)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(\"./pre-trained-model/en.wiki.bpe.op25000.vocab\")\n",
    "train_texts = [bpe.encode(s) for s in train_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from saved file \n",
    "dataset = np.load('../data/twitter/preprocessed_dataset.npz')\n",
    "\n",
    "x_train = dataset['x_train']\n",
    "y_train = dataset['y_train']\n",
    "x_test = dataset['x_test']\n",
    "y_test = dataset['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Initialization\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"./pre-trained-model/en.wiki.bpe.op25000.d50.w2v.bin\", binary=True)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "input_size = 364\n",
    "embedding_dim = 50\n",
    "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # (25001, 50)\n",
    "\n",
    "for subword, i in vocab.items():\n",
    "    if subword in model.vocab:\n",
    "        embedding_vector = model[subword]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_weights[i] = embedding_vector\n",
    "    else:\n",
    "#         print(subword) # print the subword in vocab but not in model\n",
    "        continue\n",
    "\n",
    "embedding_layer = Embedding(len(vocab)+1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
