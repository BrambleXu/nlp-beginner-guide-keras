{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import data_helpers\n",
    "\n",
    "#==================Preprocess===================\n",
    "\n",
    "# Load data\n",
    "positive_data_file = \"../data/rt-polaritydata/rt-polarity.pos\"\n",
    "negtive_data_file = \"../data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negtive_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\",\n",
       " \"the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Tokenize, Pad, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================Convert string to index================\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(x_text)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk.word_index) # 69 char, and 1 UNK token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 37,\n",
       " '!': 41,\n",
       " '\"': 45,\n",
       " '#': 51,\n",
       " '$': 52,\n",
       " '%': 53,\n",
       " '&': 55,\n",
       " \"'\": 44,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 56,\n",
       " '+': 59,\n",
       " ',': 38,\n",
       " '-': 60,\n",
       " '.': 40,\n",
       " '/': 46,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ':': 43,\n",
       " ';': 39,\n",
       " '<': 62,\n",
       " '=': 61,\n",
       " '>': 63,\n",
       " '?': 42,\n",
       " '@': 50,\n",
       " 'UNK': 70,\n",
       " '[': 66,\n",
       " '\\\\': 47,\n",
       " ']': 67,\n",
       " '^': 54,\n",
       " '_': 49,\n",
       " '`': 58,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '{': 68,\n",
       " '|': 48,\n",
       " '}': 69,\n",
       " '~': 57}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index\n",
    "sequences = tk.texts_to_sequences(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 8, 5, 37, 18, 15, 3, 11, 37, 9, 19, 37, 4, 5, 19, 20, 9, 14, 5, 4, 37, 20, 15, 37, 2, 5, 37, 20, 8, 5, 37, 29, 28, 19, 20, 37, 3, 5, 14, 20, 21, 18, 25, 37, 44, 19, 37, 14, 5, 23, 37, 3, 15, 14, 1, 14, 37, 1, 14, 4, 37, 20, 8, 1, 20, 37, 8, 5, 37, 44, 19, 37, 7, 15, 9, 14, 7, 37, 20, 15, 37, 13, 1, 11, 5, 37, 1, 37, 19, 16, 12, 1, 19, 8, 37, 5, 22, 5, 14, 37, 7, 18, 5, 1, 20, 5, 18, 37, 20, 8, 1, 14, 37, 1, 18, 14, 15, 12, 4, 37, 19, 3, 8, 23, 1, 18, 26, 5, 14, 5, 7, 7, 5, 18, 37, 38, 37, 10, 5, 1, 14, 37, 3, 12, 1, 21, 4, 37, 22, 1, 14, 37, 4, 1, 13, 13, 5, 37, 15, 18, 37, 19, 20, 5, 22, 5, 14, 37, 19, 5, 7, 1, 12], [20, 8, 5, 37, 7, 15, 18, 7, 5, 15, 21, 19, 12, 25, 37, 5, 12, 1, 2, 15, 18, 1, 20, 5, 37, 3, 15, 14, 20, 9, 14, 21, 1, 20, 9, 15, 14, 37, 15, 6, 37, 20, 8, 5, 37, 12, 15, 18, 4, 37, 15, 6, 37, 20, 8, 5, 37, 18, 9, 14, 7, 19, 37, 20, 18, 9, 12, 15, 7, 25, 37, 9, 19, 37, 19, 15, 37, 8, 21, 7, 5, 37, 20, 8, 1, 20, 37, 1, 37, 3, 15, 12, 21, 13, 14, 37, 15, 6, 37, 23, 15, 18, 4, 19, 37, 3, 1, 14, 14, 15, 20, 37, 1, 4, 5, 17, 21, 1, 20, 5, 12, 25, 37, 4, 5, 19, 3, 18, 9, 2, 5, 37, 3, 15, 37, 23, 18, 9, 20, 5, 18, 37, 4, 9, 18, 5, 3, 20, 15, 18, 37, 16, 5, 20, 5, 18, 37, 10, 1, 3, 11, 19, 15, 14, 37, 44, 19, 37, 5, 24, 16, 1, 14, 4, 5, 4, 37, 22, 9, 19, 9, 15, 14, 37, 15, 6, 37, 10, 37, 18, 37, 18, 37, 20, 15, 12, 11, 9, 5, 14, 37, 44, 19, 37, 13, 9, 4, 4, 12, 5, 37, 5, 1, 18, 20, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is:  266\n",
      "The min length is:  2\n",
      "The average length is:  111.5871318701932\n"
     ]
    }
   ],
   "source": [
    "# See char level length\n",
    "length = [len(sent) for sent in sequences]\n",
    "print('The max length is: ', max(length))\n",
    "print('The min length is: ', min(length))\n",
    "print('The average length is: ', sum(length)/len(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole data size is:  (10662, 266)\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "sequences_pad = pad_sequences(sequences, maxlen=266, padding='post')\n",
    "print(\"The whole data size is: \", sequences_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  8,  5, 37, 18, 15,  3, 11, 37,  9, 19, 37,  4,  5, 19, 20,\n",
       "         9, 14,  5,  4, 37, 20, 15, 37,  2,  5, 37, 20,  8,  5, 37, 29,\n",
       "        28, 19, 20, 37,  3,  5, 14, 20, 21, 18, 25, 37, 44, 19, 37, 14,\n",
       "         5, 23, 37,  3, 15, 14,  1, 14, 37,  1, 14,  4, 37, 20,  8,  1,\n",
       "        20, 37,  8,  5, 37, 44, 19, 37,  7, 15,  9, 14,  7, 37, 20, 15,\n",
       "        37, 13,  1, 11,  5, 37,  1, 37, 19, 16, 12,  1, 19,  8, 37,  5,\n",
       "        22,  5, 14, 37,  7, 18,  5,  1, 20,  5, 18, 37, 20,  8,  1, 14,\n",
       "        37,  1, 18, 14, 15, 12,  4, 37, 19,  3,  8, 23,  1, 18, 26,  5,\n",
       "        14,  5,  7,  7,  5, 18, 37, 38, 37, 10,  5,  1, 14, 37,  3, 12,\n",
       "         1, 21,  4, 37, 22,  1, 14, 37,  4,  1, 13, 13,  5, 37, 15, 18,\n",
       "        37, 19, 20,  5, 22,  5, 14, 37, 19,  5,  7,  1, 12,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [20,  8,  5, 37,  7, 15, 18,  7,  5, 15, 21, 19, 12, 25, 37,  5,\n",
       "        12,  1,  2, 15, 18,  1, 20,  5, 37,  3, 15, 14, 20,  9, 14, 21,\n",
       "         1, 20,  9, 15, 14, 37, 15,  6, 37, 20,  8,  5, 37, 12, 15, 18,\n",
       "         4, 37, 15,  6, 37, 20,  8,  5, 37, 18,  9, 14,  7, 19, 37, 20,\n",
       "        18,  9, 12, 15,  7, 25, 37,  9, 19, 37, 19, 15, 37,  8, 21,  7,\n",
       "         5, 37, 20,  8,  1, 20, 37,  1, 37,  3, 15, 12, 21, 13, 14, 37,\n",
       "        15,  6, 37, 23, 15, 18,  4, 19, 37,  3,  1, 14, 14, 15, 20, 37,\n",
       "         1,  4,  5, 17, 21,  1, 20,  5, 12, 25, 37,  4,  5, 19,  3, 18,\n",
       "         9,  2,  5, 37,  3, 15, 37, 23, 18,  9, 20,  5, 18, 37,  4,  9,\n",
       "        18,  5,  3, 20, 15, 18, 37, 16,  5, 20,  5, 18, 37, 10,  1,  3,\n",
       "        11, 19, 15, 14, 37, 44, 19, 37,  5, 24, 16,  1, 14,  4,  5,  4,\n",
       "        37, 22,  9, 19,  9, 15, 14, 37, 15,  6, 37, 10, 37, 18, 37, 18,\n",
       "        37, 20, 15, 12, 11,  9,  5, 14, 37, 44, 19, 37, 13,  9,  4,  4,\n",
       "        12,  5, 37,  5,  1, 18, 20,  8,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_pad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size is:  (9595, 266)\n",
      "Validation data size is:  (1067, 266)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(42)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = sequences_pad[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train and test\n",
    "training_rate = 0.9\n",
    "train_len = int(len(y) * training_rate)\n",
    "x_train = x_shuffled[:train_len]\n",
    "y_train = y_shuffled[:train_len]\n",
    "x_test = x_shuffled[train_len:]\n",
    "y_test = y_shuffled[train_len:]\n",
    "print('Training data size is: ', x_train.shape)\n",
    "print('Validation data size is: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Embedding weights with one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one-hot vector, so the embedding dim is 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding weights\n",
    "embedding_dim = 70\n",
    "\n",
    "zero_vector = np.zeros((1, embedding_dim)) \n",
    "\n",
    "embedding_weights = np.concatenate((zero_vector, np.identity(vocab_size)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_weights.shape)\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "input_size = 266\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_dim,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Character Embedding from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t -0.07856 -0.297894 -0.09703 -0.091368 -0.211625 -0.026434 0.334066 0.170385 0.097103 0.206331 0.069699 0.032036 0.285823 0.121486 -0.248542 -0.079295 -0.04549 0.085357 0.34019 0.109074 -0.107245 -0.138275 -0.114629 0.10699 0.136138 0.558307 0.077272 0.079817 0.037287 0.06284 -0.713346 -0.06032 0.201506 0.095904 -0.06107 0.114473 -0.027449 -0.031221 -0.144786 0.093606 0.124803 -0.224169 -0.118019 0.153747 -0.001278 -0.138124 0.218402 0.150867 -0.004169 0.010908\n",
      "\n",
      "h -0.161026 -0.277764 -0.056471 -0.063275 -0.194834 -0.086759 0.333846 0.205543 0.08772 0.149837 0.050109 0.045362 0.262903 0.053281 -0.268514 -0.022935 -0.020881 0.082078 0.382204 0.234935 -0.127866 -0.111021 -0.087919 0.091441 0.169804 0.560793 0.115634 0.056084 -0.066376 0.040264 -0.907454 -0.089729 0.262017 0.191351 -0.014241 0.098785 -0.020455 -0.008653 -0.074219 0.187383 0.103722 -0.120813 -0.162381 0.056297 0.082199 -0.140694 0.170395 0.07543 0.003051 0.029757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "glove_path = 'glove.6B'\n",
    "with open(os.path.join(glove_path, 'glove.6B.50d-char.txt')) as f:\n",
    "    i = 0\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "#         print(line.split())\n",
    "        i += 1\n",
    "        if i > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glove to embedding\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_path, 'glove.6B.50d-char.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t', 'h', 'e', ',', '.', 'o', 'f', 'a', 'n', 'd', 'i', '\"', \"'\", 's', 'r', '-', 'w', 'b', 'y', '(', ')', 'm', '`', 'v', 'u', 'c', 'l', ':', 'p', 'g', '$', ';', '_', 'k', 'j', '1', 'x', '?', '0', '2', 'q', '%', '/', '3', '5', '4', '8', '6', '7', '9', '&', 'z', '!', '=', '#', '[', '+', '|', ']', '~', '\\\\', '{', '>', '}', '*', '@', '<', '^'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  space\n"
     ]
    }
   ],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "for char in alphabet:\n",
    "    if char not in embeddings_index:\n",
    "        print(char, 'space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add space character and UNK in embedding_index.\n",
    "\n",
    "- space_vector: average of 68 char vector\n",
    "- unk_vector: random generate based on normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_vector = 0\n",
    "\n",
    "for char_vector in embeddings_index.values():\n",
    "    space_vector += char_vector\n",
    "space_vector /= len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29490685,  0.02992696,  0.25553298, -0.00351198, -0.06802648,\n",
       "       -0.16345023,  0.47377503, -0.00430326, -0.10446006,  0.10570163,\n",
       "       -0.09848271, -0.00441733,  0.36036023, -0.05057086, -0.06766081,\n",
       "       -0.10361586, -0.14869866,  0.07603326,  0.11113335,  0.1667211 ,\n",
       "       -0.13753006, -0.13342041,  0.21635625,  0.17358167,  0.14035112,\n",
       "        0.6127235 ,  0.05064495, -0.02979143,  0.20813598,  0.02590212,\n",
       "       -0.353355  ,  0.04656252,  0.07981671,  0.3142017 , -0.06188979,\n",
       "       -0.06343918,  0.15323772, -0.23385583,  0.09215084,  0.01510967,\n",
       "        0.28810412, -0.3465999 ,  0.05195587,  0.20819926, -0.15522721,\n",
       "        0.0526934 ,  0.30569547,  0.15746509,  0.13995562,  0.23179682],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11851034,  0.02811158,  0.08536472, -0.04697181,  0.01525296,\n",
       "        0.17150421, -0.11922493,  0.08820908,  0.10156986, -0.07266301,\n",
       "        0.01444775, -0.10209685, -0.14609937,  0.06401705, -0.08057213,\n",
       "       -0.11534708,  0.18761204,  0.08609939,  0.21568493, -0.1507021 ,\n",
       "        0.02629094,  0.10310304, -0.09095152,  0.11523869,  0.11691516,\n",
       "       -0.01026392, -0.10387156,  0.02612882,  0.0090515 ,  0.07139222,\n",
       "       -0.06237833,  0.07958395,  0.03694744,  0.01692949, -0.00929912,\n",
       "       -0.00189833,  0.07110632, -0.10874058,  0.13199082, -0.08803322,\n",
       "        0.06132961, -0.08203701, -0.00162088, -0.08185927,  0.05843014,\n",
       "        0.00386503, -0.06305137, -0.18833562, -0.04731349, -0.23160425])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "unk_vector = np.random.normal(mu, sigma, 50)\n",
    "unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add space and unknown vector to embedding_index\n",
    "embeddings_index[' '] = space_vector\n",
    "embeddings_index['UNK'] = unk_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "char_dict['UNK'] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 37,\n",
       " '!': 41,\n",
       " '\"': 45,\n",
       " '#': 51,\n",
       " '$': 52,\n",
       " '%': 53,\n",
       " '&': 55,\n",
       " \"'\": 44,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 56,\n",
       " '+': 59,\n",
       " ',': 38,\n",
       " '-': 60,\n",
       " '.': 40,\n",
       " '/': 46,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ':': 43,\n",
       " ';': 39,\n",
       " '<': 62,\n",
       " '=': 61,\n",
       " '>': 63,\n",
       " '?': 42,\n",
       " '@': 50,\n",
       " 'UNK': 70,\n",
       " '[': 66,\n",
       " '\\\\': 47,\n",
       " ']': 67,\n",
       " '^': 54,\n",
       " '_': 49,\n",
       " '`': 58,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '{': 68,\n",
       " '|': 48,\n",
       " '}': 69,\n",
       " '~': 57}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glove to embedding\n",
    "glove_path = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_path, 'glove.6B.50d-char.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Add space and unknown vector to embeddings_index\n",
    "space_vector = 0\n",
    "for char_vector in embeddings_index.values():\n",
    "    space_vector += char_vector\n",
    "space_vector /= len(embeddings_index)\n",
    "\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "unk_vector = np.random.normal(mu, sigma, 50)\n",
    "\n",
    "embeddings_index[' '] = space_vector\n",
    "embeddings_index['UNK'] = unk_vector # len(embeddings_index) == 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06618765,  0.0264331 , -0.0065112 ,  0.11983904, -0.10909893,\n",
       "        0.04808326,  0.01033974,  0.13337676,  0.03288751,  0.06613451,\n",
       "        0.02635759,  0.09601076,  0.02943245, -0.00462533, -0.00306213,\n",
       "        0.16339628, -0.04545588, -0.1157389 ,  0.13620139, -0.01573983,\n",
       "        0.29609983, -0.03079412,  0.03417422,  0.10646769, -0.10084371,\n",
       "       -0.08748211,  0.0178019 ,  0.2071987 ,  0.08209752,  0.00147916,\n",
       "        0.03178311,  0.08249312, -0.05654057,  0.06690726, -0.21578571,\n",
       "        0.05159405, -0.05221445,  0.15134824,  0.04097625, -0.06370345,\n",
       "        0.05106042, -0.13223772,  0.10402735,  0.08573347, -0.05261663,\n",
       "        0.05478117, -0.19624598, -0.04386312, -0.06730313, -0.08794197])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding weights\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((len(char_dict)+1, embedding_dim)) # fist row represent padding with 0, 71x50\n",
    "for char, i in char_dict.items():  # tk.word_index contain 69 char\n",
    "    embedding_vector = embeddings_index.get(char) # if not find in the dict, return None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else: # For the unknown word in tk.word_index, assign UNK vector\n",
    "        embedding_vector = embeddings_index.get('UNK')\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.101148  , -0.31064701, -0.096601  , ...,  0.092758  ,\n",
       "         0.036486  ,  0.006396  ],\n",
       "       [-0.124808  , -0.308925  , -0.062367  , ...,  0.093737  ,\n",
       "        -0.006683  ,  0.024481  ],\n",
       "       ...,\n",
       "       [-0.42855   ,  1.05509996,  0.60421002, ..., -0.075322  ,\n",
       "        -0.13569   ,  0.61049998],\n",
       "       [-0.67074001,  0.69856   ,  0.69630003, ...,  0.080127  ,\n",
       "         0.10094   ,  0.92917001],\n",
       "       [-0.06618765,  0.0264331 , -0.0065112 , ..., -0.04386312,\n",
       "        -0.06730313, -0.08794197]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
